{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea64d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "from sqlalchemy.types import Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f6a20",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab7e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/DGA.xlsx\n",
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA   DGA\n",
      "0    146660T3 2022-05-12  2.65\n",
      "1    146660T3 2017-03-10  2.30\n",
      "2    146660T3 2015-06-19  1.75\n",
      "3    146660T3 2013-10-22  1.90\n",
      "4      364076 2022-06-18  1.75\n",
      "5      364076 2017-06-11  2.35\n",
      "6      364076 2016-01-15  1.30\n",
      "7      364076 2014-11-15  2.65\n",
      "8   230531-01 2024-07-16  2.80\n",
      "9   230531-01 2016-11-15  3.95\n",
      "10  230531-01 2015-03-29  3.55\n",
      "11  230531-01 2014-06-10  1.90\n",
      "12     338118 2024-11-13  2.35\n",
      "13     338118 2016-12-19  2.35\n",
      "14     338118 2015-01-27  2.10\n",
      "15     338118 2013-12-13  1.30 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  DGA\n",
      "0  338118 2015-01-01  1.3\n",
      "1  338118 2015-01-02  1.3\n",
      "2  338118 2015-01-03  1.3\n",
      "3  338118 2015-01-04  1.3\n",
      "4  338118 2015-01-05  1.3 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES DE DGA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA   DGA   H2  CH4  C2H2  C2H4  C2H6    CO    CO2     O2\n",
      "0  146660T3 2022-05-12  2.65  179   53    17   314   139  1491   5095  10843\n",
      "1  146660T3 2017-03-10  2.30   75   42    29   134    10   962   1588  16321\n",
      "2  146660T3 2015-06-19  1.75  175  118     8   177    26   858  14735  15477\n",
      "3  146660T3 2013-10-22  1.90  163  264    17    21    30   537  15494   3637\n",
      "4    364076 2022-06-18  1.75   90   71     1   332    89   444  16063  19419 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES DE DGA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  DGA     H2   CH4  C2H2   C2H4  C2H6    CO     CO2  \\\n",
      "0  338118 2015-01-01  1.3  191.0  36.0  11.0  222.0  71.0  97.0  3369.0   \n",
      "1  338118 2015-01-02  1.3  191.0  36.0  11.0  222.0  71.0  97.0  3369.0   \n",
      "2  338118 2015-01-03  1.3  191.0  36.0  11.0  222.0  71.0  97.0  3369.0   \n",
      "3  338118 2015-01-04  1.3  191.0  36.0  11.0  222.0  71.0  97.0  3369.0   \n",
      "4  338118 2015-01-05  1.3  191.0  36.0  11.0  222.0  71.0  97.0  3369.0   \n",
      "\n",
      "       O2  \n",
      "0  1009.0  \n",
      "1  1009.0  \n",
      "2  1009.0  \n",
      "3  1009.0  \n",
      "4  1009.0   \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/ACE.xlsx\n",
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA       ACE\n",
      "0    146660T3 2022-05-12  4.090909\n",
      "1    146660T3 2018-09-17  3.636364\n",
      "2    146660T3 2015-08-19  2.909091\n",
      "3    146660T3 2013-02-19  3.545455\n",
      "4      364076 2022-06-18  2.181818\n",
      "5      364076 2021-08-06  4.000000\n",
      "6      364076 2017-12-06  3.181818\n",
      "7      364076 2012-10-19  4.272727\n",
      "8   230531-01 2024-07-16  4.000000\n",
      "9   230531-01 2021-01-29  3.454545\n",
      "10  230531-01 2018-01-16  3.545455\n",
      "11  230531-01 2013-01-03  3.090909\n",
      "12     338118 2024-11-13  2.363636\n",
      "13     338118 2017-06-15  3.727273\n",
      "14     338118 2015-02-24  3.181818\n",
      "15     338118 2014-08-27  4.181818\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA       ACE\n",
      "0  338118 2015-01-01  4.181818\n",
      "1  338118 2015-01-02  4.181818\n",
      "2  338118 2015-01-03  4.181818\n",
      "3  338118 2015-01-04  4.181818\n",
      "4  338118 2015-01-05  4.181818\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA       ACE  FP25  FP100  HU    AC  TIF  CO  RD    IO\n",
      "0  146660T3 2022-05-12  4.090909   1.3      7  91  0.06   41   6  31  0.31\n",
      "1  146660T3 2018-09-17  3.636364   0.3      9  41  0.07   43   2  15  0.25\n",
      "2  146660T3 2015-08-19  2.909091   0.3      1  77  0.06   31   3  38  0.26\n",
      "3  146660T3 2013-02-19  3.545455   0.6      5  63  0.02   29   0  39  0.35\n",
      "4    364076 2022-06-18  2.181818   0.1      4  43  0.08   41   3  46  0.21\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA       ACE  FP25  FP100    HU    AC   TIF   CO    RD   IO\n",
      "0  338118 2015-01-01  4.181818   0.7    5.0  63.0  0.13  39.0  3.0  27.0  0.1\n",
      "1  338118 2015-01-02  4.181818   0.7    5.0  63.0  0.13  39.0  3.0  27.0  0.1\n",
      "2  338118 2015-01-03  4.181818   0.7    5.0  63.0  0.13  39.0  3.0  27.0  0.1\n",
      "3  338118 2015-01-04  4.181818   0.7    5.0  63.0  0.13  39.0  3.0  27.0  0.1\n",
      "4  338118 2015-01-05  4.181818   0.7    5.0  63.0  0.13  39.0  3.0  27.0  0.1\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/RDEV.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:62: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[f\"tasa_{gas}\"] = df.groupby(\"SERIE\").apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:116: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:116: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:124: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\DGA.py:124: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ACE.py:148: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ACE.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ACE.py:155: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ACE.py:155: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[puntaje_col] = df[col].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"ROHM\"] = df[score_cols].max(axis=1)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:89: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:89: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:96: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrohm.py:96: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  ROHM\n",
      "0    146660T3 2022-05-12   5.0\n",
      "1    146660T3 2021-07-08   5.0\n",
      "2    146660T3 2019-09-09   5.0\n",
      "3    146660T3 2018-03-03   1.0\n",
      "4      364076 2022-06-18   5.0\n",
      "5      364076 2019-01-12   5.0\n",
      "6      364076 2017-06-13   5.0\n",
      "7      364076 2015-01-10   1.0\n",
      "8   230531-01 2024-07-16   5.0\n",
      "9   230531-01 2017-10-16   5.0\n",
      "10  230531-01 2016-08-08   5.0\n",
      "11  230531-01 2015-12-31   1.0\n",
      "12     338118 2024-11-13   5.0\n",
      "13     338118 2020-09-18   5.0\n",
      "14     338118 2018-08-23   5.0\n",
      "15     338118 2016-11-07   1.0 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  ROHM\n",
      "0  338118 2015-01-01   NaN\n",
      "1  338118 2015-01-02   NaN\n",
      "2  338118 2015-01-03   NaN\n",
      "3  338118 2015-01-04   NaN\n",
      "4  338118 2015-01-05   NaN \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  ROHM  H POS1 R 75°C [mΩ]  H POS2 R 75°C [mΩ]  \\\n",
      "0  146660T3 2022-05-12   5.0             119.972             144.530   \n",
      "1  146660T3 2021-07-08   5.0             119.547             151.467   \n",
      "2  146660T3 2019-09-09   5.0             135.782             134.007   \n",
      "3  146660T3 2018-03-03   1.0             130.783             147.966   \n",
      "4    364076 2022-06-18   5.0             134.255             123.959   \n",
      "\n",
      "   H POS3 R 75°C [mΩ]  H POS4 R 75°C [mΩ]  H POS5 R 75°C [mΩ]  \\\n",
      "0             141.528             126.792             129.259   \n",
      "1             148.022             150.973             128.435   \n",
      "2             105.708             118.009             128.354   \n",
      "3             143.770             111.817             139.191   \n",
      "4             142.857             111.972             114.735   \n",
      "\n",
      "   H POS6 R 75°C [mΩ]  H POS7 R 75°C [mΩ]  ...  H POS24 T 75°C [mΩ]  \\\n",
      "0             133.276             145.122  ...              139.530   \n",
      "1             159.888             156.112  ...              114.972   \n",
      "2             138.942             157.732  ...              142.943   \n",
      "3             140.366             123.187  ...              109.539   \n",
      "4             128.979             131.346  ...              116.811   \n",
      "\n",
      "   H POS25 T 75°C [mΩ]  H POS26 T 75°C [mΩ]  H POS27 T 75°C [mΩ]  \\\n",
      "0              117.181              105.375              143.185   \n",
      "1              158.177              131.723              152.473   \n",
      "2              157.936              136.143              137.432   \n",
      "3              121.162              126.707              132.010   \n",
      "4              141.326              156.980              106.554   \n",
      "\n",
      "   X POS1 R 75°C[mΩ]  X POS1 S 75°C[mΩ]  X POS1 T 75°C[mΩ]  Y POS1 R 75°C[mΩ]  \\\n",
      "0             27.998              7.492             25.014                NaN   \n",
      "1             78.062             38.596             17.474                NaN   \n",
      "2              6.049             30.624             75.912                NaN   \n",
      "3             76.903             47.650             88.398                NaN   \n",
      "4             97.623             71.297             65.690                NaN   \n",
      "\n",
      "   Y POS1 S 75°C[mΩ]  Y POS1 T 75°C[mΩ]  \n",
      "0                NaN                NaN  \n",
      "1                NaN                NaN  \n",
      "2                NaN                NaN  \n",
      "3                NaN                NaN  \n",
      "4                NaN                NaN  \n",
      "\n",
      "[5 rows x 90 columns] \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  ROHM  H POS1 R 75°C [mΩ]  H POS2 R 75°C [mΩ]  \\\n",
      "0  338118 2015-01-01   NaN                 NaN                 NaN   \n",
      "1  338118 2015-01-02   NaN                 NaN                 NaN   \n",
      "2  338118 2015-01-03   NaN                 NaN                 NaN   \n",
      "3  338118 2015-01-04   NaN                 NaN                 NaN   \n",
      "4  338118 2015-01-05   NaN                 NaN                 NaN   \n",
      "\n",
      "   H POS3 R 75°C [mΩ]  H POS4 R 75°C [mΩ]  H POS5 R 75°C [mΩ]  \\\n",
      "0                 NaN                 NaN                 NaN   \n",
      "1                 NaN                 NaN                 NaN   \n",
      "2                 NaN                 NaN                 NaN   \n",
      "3                 NaN                 NaN                 NaN   \n",
      "4                 NaN                 NaN                 NaN   \n",
      "\n",
      "   H POS6 R 75°C [mΩ]  H POS7 R 75°C [mΩ]  ...  H POS24 T 75°C [mΩ]  \\\n",
      "0                 NaN                 NaN  ...                  NaN   \n",
      "1                 NaN                 NaN  ...                  NaN   \n",
      "2                 NaN                 NaN  ...                  NaN   \n",
      "3                 NaN                 NaN  ...                  NaN   \n",
      "4                 NaN                 NaN  ...                  NaN   \n",
      "\n",
      "   H POS25 T 75°C [mΩ]  H POS26 T 75°C [mΩ]  H POS27 T 75°C [mΩ]  \\\n",
      "0                  NaN                  NaN                  NaN   \n",
      "1                  NaN                  NaN                  NaN   \n",
      "2                  NaN                  NaN                  NaN   \n",
      "3                  NaN                  NaN                  NaN   \n",
      "4                  NaN                  NaN                  NaN   \n",
      "\n",
      "   X POS1 R 75°C[mΩ]  X POS1 S 75°C[mΩ]  X POS1 T 75°C[mΩ]  Y POS1 R 75°C[mΩ]  \\\n",
      "0                NaN                NaN                NaN                NaN   \n",
      "1                NaN                NaN                NaN                NaN   \n",
      "2                NaN                NaN                NaN                NaN   \n",
      "3                NaN                NaN                NaN                NaN   \n",
      "4                NaN                NaN                NaN                NaN   \n",
      "\n",
      "   Y POS1 S 75°C[mΩ]  Y POS1 T 75°C[mΩ]  \n",
      "0                NaN                NaN  \n",
      "1                NaN                NaN  \n",
      "2                NaN                NaN  \n",
      "3                NaN                NaN  \n",
      "4                NaN                NaN  \n",
      "\n",
      "[5 rows x 90 columns] \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/RTRA.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_Delta\"] = delta\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"Max_Delta\"] = df[delta_cols].max(axis=1)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"RTRA\"] = df[\"Max_Delta\"].apply(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:79: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida = df_extendida.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:86: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRrtra.py:86: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_detalles = df_extendida_detalles.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  RTRA\n",
      "0    146660T3 2022-05-12     5\n",
      "1    146660T3 2019-09-09     5\n",
      "2    146660T3 2018-06-04     5\n",
      "3    146660T3 2017-04-02     1\n",
      "4      364076 2022-06-18     5\n",
      "5      364076 2019-09-22     5\n",
      "6      364076 2018-09-29     5\n",
      "7      364076 2015-01-16     1\n",
      "8   230531-01 2024-07-16     5\n",
      "9   230531-01 2018-10-25     5\n",
      "10  230531-01 2015-01-19     5\n",
      "11  230531-01 2013-12-21     1\n",
      "12     338118 2024-11-13     5\n",
      "13     338118 2018-09-10     5\n",
      "14     338118 2016-11-02     5\n",
      "15     338118 2015-07-10     1 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RTRA\n",
      "0  338118 2015-01-01   NaN\n",
      "1  338118 2015-01-02   NaN\n",
      "2  338118 2015-01-03   NaN\n",
      "3  338118 2015-01-04   NaN\n",
      "4  338118 2015-01-05   NaN \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  RTRA  H-X POS1 R  H-X POS2 R  H-X POS3 R  H-X POS4 R  \\\n",
      "0  146660T3 2022-05-12     5    3.768769    3.744405    3.707801    3.671139   \n",
      "1  146660T3 2019-09-09     5    3.771300    3.746000    3.709000    3.672300   \n",
      "2  146660T3 2018-06-04     5    3.772695    3.740075    3.707455    3.674834   \n",
      "3  146660T3 2017-04-02     1    2.852500    2.824700    2.796900    2.778400   \n",
      "4    364076 2022-06-18     5    2.426000    3.805000    3.502000    3.901000   \n",
      "\n",
      "   H-X POS5 R  H-X POS6 R  H-X POS7 R  ...  H-Y POS21 T  H-Y POS22 T  \\\n",
      "0    3.646833    3.610171    3.573509  ...          NaN          NaN   \n",
      "1    3.647800    3.611200    3.574500  ...          NaN          NaN   \n",
      "2    3.642214    3.609594    3.576974  ...          NaN          NaN   \n",
      "3    2.750500    2.722700    2.704100  ...          NaN          NaN   \n",
      "4    3.591000    2.533000    2.620000  ...          NaN          NaN   \n",
      "\n",
      "   H-Y POS23 T  H-Y POS24 T  H-Y POS25 T  H-Y POS26 T  H-Y POS27 T  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "1          NaN          NaN          NaN          NaN          NaN   \n",
      "2          NaN          NaN          NaN          NaN          NaN   \n",
      "3          NaN          NaN          NaN          NaN          NaN   \n",
      "4          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   X-Y POS1 R  X-Y POS1 S  X-Y POS1 T  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 168 columns] \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RTRA  H-X POS1 R  H-X POS2 R  H-X POS3 R  H-X POS4 R  \\\n",
      "0  338118 2015-01-01   NaN         NaN         NaN         NaN         NaN   \n",
      "1  338118 2015-01-02   NaN         NaN         NaN         NaN         NaN   \n",
      "2  338118 2015-01-03   NaN         NaN         NaN         NaN         NaN   \n",
      "3  338118 2015-01-04   NaN         NaN         NaN         NaN         NaN   \n",
      "4  338118 2015-01-05   NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   H-X POS5 R  H-X POS6 R  H-X POS7 R  ...  H-Y POS21 T  H-Y POS22 T  \\\n",
      "0         NaN         NaN         NaN  ...          NaN          NaN   \n",
      "1         NaN         NaN         NaN  ...          NaN          NaN   \n",
      "2         NaN         NaN         NaN  ...          NaN          NaN   \n",
      "3         NaN         NaN         NaN  ...          NaN          NaN   \n",
      "4         NaN         NaN         NaN  ...          NaN          NaN   \n",
      "\n",
      "   H-Y POS23 T  H-Y POS24 T  H-Y POS25 T  H-Y POS26 T  H-Y POS27 T  \\\n",
      "0          NaN          NaN          NaN          NaN          NaN   \n",
      "1          NaN          NaN          NaN          NaN          NaN   \n",
      "2          NaN          NaN          NaN          NaN          NaN   \n",
      "3          NaN          NaN          NaN          NaN          NaN   \n",
      "4          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   X-Y POS1 R  X-Y POS1 S  X-Y POS1 T  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 168 columns] \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/RDIS.xlsx\n",
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  RDIS\n",
      "0  146660T3 2022-05-12   5.0\n",
      "1  146660T3 2018-02-20   5.0\n",
      "2  146660T3 2016-07-31   5.0\n",
      "3  146660T3 2015-06-06   1.0\n",
      "4    364076 2022-06-18   5.0 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RDIS\n",
      "0  338118 2015-01-01   5.0\n",
      "1  338118 2015-01-02   5.0\n",
      "2  338118 2015-01-03   5.0\n",
      "3  338118 2015-01-04   5.0\n",
      "4  338118 2015-01-05   5.0 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  RDIS  H-X 1 [%] ONAF 2  H-X 14 [%] ONAF 2  \\\n",
      "0  146660T3 2022-05-12   5.0            11.720             11.070   \n",
      "1  146660T3 2018-02-20   5.0            11.438              5.847   \n",
      "2  146660T3 2016-07-31   5.0             6.925             11.276   \n",
      "3  146660T3 2015-06-06   1.0             5.967             11.012   \n",
      "4    364076 2022-06-18   5.0             8.346              9.233   \n",
      "\n",
      "   H-X 27 [%] ONAF 2  H-Y 1 [%]  H-Y 14 [%]  H-Y 27 [%]  X-Y 1 [%]  X-Y 2 [%]  \\\n",
      "0             10.670        NaN         NaN         NaN        NaN        NaN   \n",
      "1              9.928        NaN         NaN         NaN        NaN        NaN   \n",
      "2              7.360        NaN         NaN         NaN        NaN        NaN   \n",
      "3              6.140        NaN         NaN         NaN        NaN        NaN   \n",
      "4              8.207        NaN         NaN         NaN        NaN        NaN   \n",
      "\n",
      "   X-Y 3 [%]  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN   \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RDIS  H-X 1 [%] ONAF 2  H-X 14 [%] ONAF 2  \\\n",
      "0  338118 2015-01-01   5.0            10.626              8.988   \n",
      "1  338118 2015-01-02   5.0            10.626              8.988   \n",
      "2  338118 2015-01-03   5.0            10.626              8.988   \n",
      "3  338118 2015-01-04   5.0            10.626              8.988   \n",
      "4  338118 2015-01-05   5.0            10.626              8.988   \n",
      "\n",
      "   H-X 27 [%] ONAF 2  H-Y 1 [%]  H-Y 14 [%]  H-Y 27 [%]  X-Y 1 [%]  X-Y 2 [%]  \\\n",
      "0              5.983        NaN         NaN         NaN        NaN        NaN   \n",
      "1              5.983        NaN         NaN         NaN        NaN        NaN   \n",
      "2              5.983        NaN         NaN         NaN        NaN        NaN   \n",
      "3              5.983        NaN         NaN         NaN        NaN        NaN   \n",
      "4              5.983        NaN         NaN         NaN        NaN        NaN   \n",
      "\n",
      "   X-Y 3 [%]  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:79: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_DIS_ext = df_DIS_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_DIS_ext = df_DIS_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:91: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\ARRdis.py:91: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SERIE      FECHA  ARR  ROHM  RTRA  RDIS\n",
      "0      338118 2013-09-15  1.0   NaN   NaN   1.0\n",
      "1      338118 2014-04-14  5.0   NaN   NaN   5.0\n",
      "2      338118 2015-02-24  5.0   NaN   NaN   5.0\n",
      "3      338118 2015-07-10  1.0   NaN   1.0   NaN\n",
      "4      338118 2016-11-02  5.0   NaN   5.0   NaN\n",
      "5      338118 2016-11-07  1.0   1.0   NaN   NaN\n",
      "6      338118 2018-08-23  5.0   5.0   NaN   NaN\n",
      "7      338118 2018-09-10  5.0   NaN   5.0   NaN\n",
      "8      338118 2020-09-18  5.0   5.0   NaN   NaN\n",
      "9      338118 2024-11-13  5.0   5.0   5.0   5.0\n",
      "10     364076 2013-12-30  1.0   NaN   NaN   1.0\n",
      "11     364076 2015-01-10  1.0   1.0   NaN   NaN\n",
      "12     364076 2015-01-16  1.0   NaN   1.0   NaN\n",
      "13     364076 2015-08-03  5.0   NaN   NaN   5.0\n",
      "14     364076 2017-06-13  5.0   5.0   NaN   NaN\n",
      "15     364076 2017-11-30  5.0   NaN   NaN   5.0\n",
      "16     364076 2018-09-29  5.0   NaN   5.0   NaN\n",
      "17     364076 2019-01-12  5.0   5.0   NaN   NaN\n",
      "18     364076 2019-09-22  5.0   NaN   5.0   NaN\n",
      "19     364076 2022-06-18  5.0   5.0   5.0   5.0\n",
      "20   146660T3 2015-06-06  1.0   NaN   NaN   1.0\n",
      "21   146660T3 2016-07-31  5.0   NaN   NaN   5.0\n",
      "22   146660T3 2017-04-02  1.0   NaN   1.0   NaN\n",
      "23   146660T3 2018-02-20  5.0   NaN   NaN   5.0\n",
      "24   146660T3 2018-03-03  1.0   1.0   NaN   NaN\n",
      "25   146660T3 2018-06-04  5.0   NaN   5.0   NaN\n",
      "26   146660T3 2019-09-09  5.0   5.0   5.0   NaN\n",
      "27   146660T3 2021-07-08  5.0   5.0   NaN   NaN\n",
      "28   146660T3 2022-05-12  5.0   5.0   5.0   5.0\n",
      "29  230531-01 2013-09-08  1.0   NaN   NaN   1.0\n",
      "30  230531-01 2013-12-21  1.0   NaN   1.0   NaN\n",
      "31  230531-01 2015-01-19  5.0   NaN   5.0   NaN\n",
      "32  230531-01 2015-03-08  5.0   NaN   NaN   5.0\n",
      "33  230531-01 2015-12-31  1.0   1.0   NaN   NaN\n",
      "34  230531-01 2016-08-08  5.0   5.0   NaN   NaN\n",
      "35  230531-01 2017-10-16  5.0   5.0   NaN   NaN\n",
      "36  230531-01 2018-04-14  5.0   NaN   NaN   5.0\n",
      "37  230531-01 2018-10-25  5.0   NaN   5.0   NaN\n",
      "38  230531-01 2024-07-16  5.0   5.0   5.0   5.0\n",
      "           SERIE      FECHA  ARR  ROHM  RTRA  RDIS\n",
      "0         338118 2015-01-01  5.0   NaN   NaN   5.0\n",
      "1         338118 2015-01-02  5.0   NaN   NaN   5.0\n",
      "2         338118 2015-01-03  5.0   NaN   NaN   5.0\n",
      "3         338118 2015-01-04  5.0   NaN   NaN   5.0\n",
      "4         338118 2015-01-05  5.0   NaN   NaN   5.0\n",
      "...          ...        ...  ...   ...   ...   ...\n",
      "15723  230531-01 2025-10-02  5.0   5.0   5.0   5.0\n",
      "15724  230531-01 2025-10-03  5.0   5.0   5.0   5.0\n",
      "15725  230531-01 2025-10-04  5.0   5.0   5.0   5.0\n",
      "15726  230531-01 2025-10-05  5.0   5.0   5.0   5.0\n",
      "15727  230531-01 2025-10-06  5.0   5.0   5.0   5.0\n",
      "\n",
      "[15728 rows x 6 columns]\n",
      "           SERIE      FECHA  ARR\n",
      "0         338118 2015-01-01  5.0\n",
      "1         338118 2015-01-02  5.0\n",
      "2         338118 2015-01-03  5.0\n",
      "3         338118 2015-01-04  5.0\n",
      "4         338118 2015-01-05  5.0\n",
      "...          ...        ...  ...\n",
      "15723  230531-01 2025-10-02  5.0\n",
      "15724  230531-01 2025-10-03  5.0\n",
      "15725  230531-01 2025-10-04  5.0\n",
      "15726  230531-01 2025-10-05  5.0\n",
      "15727  230531-01 2025-10-06  5.0\n",
      "\n",
      "[15728 rows x 3 columns]\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPDEVANADO.xlsx\n",
      "\n",
      " ====== TABLA FPDEVANADO ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA  FPDEVANADO\n",
      "0    146660T3 2022-05-12         5.0\n",
      "1    146660T3 2018-05-21         5.0\n",
      "2    146660T3 2015-05-28         5.0\n",
      "3    146660T3 2013-09-30         5.0\n",
      "4      364076 2022-06-18         5.0\n",
      "5      364076 2020-03-07         5.0\n",
      "6      364076 2016-02-28         5.0\n",
      "7      364076 2014-05-02         5.0\n",
      "8   230531-01 2024-07-16         5.0\n",
      "9   230531-01 2016-06-21         5.0\n",
      "10  230531-01 2015-06-18         5.0\n",
      "11  230531-01 2014-01-20         5.0\n",
      "12     338118 2024-11-13         5.0\n",
      "13     338118 2018-08-29         5.0\n",
      "14     338118 2017-12-18         5.0\n",
      "15     338118 2016-09-02         5.0 \n",
      "\n",
      "\n",
      " ====== TABLA FPDEVANADO EXTENDIDA ====== \n",
      "\n",
      "    SERIE      FECHA  FPDEVANADO\n",
      "0  338118 2015-01-01         NaN\n",
      "1  338118 2015-01-02         NaN\n",
      "2  338118 2015-01-03         NaN\n",
      "3  338118 2015-01-04         NaN\n",
      "4  338118 2015-01-05         NaN \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA FPDEVANADO ====== \n",
      "\n",
      "      SERIE      FECHA  H ICH+ICHL %  H ICH %  H ICHL %  H ICHT %  \\\n",
      "0  146660T3 2022-05-12         0.239    0.960     0.303     0.942   \n",
      "1  146660T3 2018-05-21         1.023    0.648     1.306     0.850   \n",
      "2  146660T3 2015-05-28         0.454    0.376     1.237     1.065   \n",
      "3  146660T3 2013-09-30         0.516    0.921     0.449     0.691   \n",
      "4    364076 2022-06-18         1.290    0.552     0.614     1.008   \n",
      "\n",
      "   X ICL+ICLT+ICLH %  X ICL+ICLT %  X ICL %  X ICLT %  X ICLH %  \\\n",
      "0              0.834         1.021    0.518     0.956     0.609   \n",
      "1              0.920         0.263    0.627     1.030     1.084   \n",
      "2              0.245         1.202    0.595     0.262     1.009   \n",
      "3              0.124         1.236    0.451     0.124     0.295   \n",
      "4              1.241         0.204    0.238     0.840     0.310   \n",
      "\n",
      "   Y ICT+ICTH+ICTL %  Y ICT+ICTH %  Y ICT %  Y ICTH %  Y ICTL %  FPDEVANADO  \n",
      "0              0.866         1.131    0.909     0.770     1.093         5.0  \n",
      "1              0.329         1.144    0.302     0.166     0.811         5.0  \n",
      "2              0.312         1.055    0.516     0.169     0.836         5.0  \n",
      "3              0.399         0.644    1.300     0.797     0.636         5.0  \n",
      "4              0.212         0.387    0.458     0.339     0.188         5.0   \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA FPDEVANADO ====== \n",
      "\n",
      "    SERIE      FECHA  H ICH+ICHL %  H ICH %  H ICHL %  H ICHT %  \\\n",
      "0  338118 2015-01-01           NaN      NaN       NaN       NaN   \n",
      "1  338118 2015-01-02           NaN      NaN       NaN       NaN   \n",
      "2  338118 2015-01-03           NaN      NaN       NaN       NaN   \n",
      "3  338118 2015-01-04           NaN      NaN       NaN       NaN   \n",
      "4  338118 2015-01-05           NaN      NaN       NaN       NaN   \n",
      "\n",
      "   X ICL+ICLT+ICLH %  X ICL+ICLT %  X ICL %  X ICLT %  X ICLH %  \\\n",
      "0                NaN           NaN      NaN       NaN       NaN   \n",
      "1                NaN           NaN      NaN       NaN       NaN   \n",
      "2                NaN           NaN      NaN       NaN       NaN   \n",
      "3                NaN           NaN      NaN       NaN       NaN   \n",
      "4                NaN           NaN      NaN       NaN       NaN   \n",
      "\n",
      "   Y ICT+ICTH+ICTL %  Y ICT+ICTH %  Y ICT %  Y ICTH %  Y ICTL %  FPDEVANADO  \n",
      "0                NaN           NaN      NaN       NaN       NaN         NaN  \n",
      "1                NaN           NaN      NaN       NaN       NaN         NaN  \n",
      "2                NaN           NaN      NaN       NaN       NaN         NaN  \n",
      "3                NaN           NaN      NaN       NaN       NaN         NaN  \n",
      "4                NaN           NaN      NaN       NaN       NaN         NaN   \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPDEVANADO.xlsx\n",
      "\n",
      " ====== TABLA CD ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA   CD\n",
      "0    146660T3 2022-05-12  5.0\n",
      "1    146660T3 2018-05-21  5.0\n",
      "2    146660T3 2015-05-28  5.0\n",
      "3    146660T3 2013-09-30  1.0\n",
      "4      364076 2022-06-18  5.0\n",
      "5      364076 2020-03-07  5.0\n",
      "6      364076 2016-02-28  5.0\n",
      "7      364076 2014-05-02  1.0\n",
      "8   230531-01 2024-07-16  5.0\n",
      "9   230531-01 2016-06-21  5.0\n",
      "10  230531-01 2015-06-18  5.0\n",
      "11  230531-01 2014-01-20  1.0\n",
      "12     338118 2024-11-13  5.0\n",
      "13     338118 2018-08-29  5.0\n",
      "14     338118 2017-12-18  5.0\n",
      "15     338118 2016-09-02  1.0 \n",
      "\n",
      "\n",
      " ====== TABLA CD EXTENDIDA ====== \n",
      "\n",
      "    SERIE      FECHA  CD\n",
      "0  338118 2015-01-01 NaN\n",
      "1  338118 2015-01-02 NaN\n",
      "2  338118 2015-01-03 NaN\n",
      "3  338118 2015-01-04 NaN\n",
      "4  338118 2015-01-05 NaN \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA CD ====== \n",
      "\n",
      "      SERIE      FECHA   CD  H ICH+ICHL pF  H ICH pF  H ICHL pF  H ICHT pF  \\\n",
      "0  146660T3 2022-05-12  5.0            887     14294       2385       3283   \n",
      "1  146660T3 2018-05-21  5.0           3266     14844       4973       4378   \n",
      "2  146660T3 2015-05-28  5.0          13595      3041      13409       4997   \n",
      "3  146660T3 2013-09-30  1.0           3575      5165       9934       2680   \n",
      "4    364076 2022-06-18  5.0          10026     10600       2284       9280   \n",
      "\n",
      "   X ICL+ICLT pF  X ICL pF  X ICLT pF  X ICLH pF  Y ICT+ICTH pF  Y ICT pF  \\\n",
      "0           9684      5342       3452       2346           9356     11655   \n",
      "1           7656      6827       6171       1473           1099     14150   \n",
      "2           3363      9453       4502       9393           1027     11178   \n",
      "3           3167      2562       9573       2551           7384     13682   \n",
      "4           4685      2261       9140        975           6427      7625   \n",
      "\n",
      "   Y ICTH pF  Y ICTL pF  \n",
      "0       2163      14602  \n",
      "1      10186       6119  \n",
      "2       1726       6417  \n",
      "3      12874      13864  \n",
      "4      11516      10686   \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA CD ====== \n",
      "\n",
      "    SERIE      FECHA  CD  H ICH+ICHL pF  H ICH pF  H ICHL pF  H ICHT pF  \\\n",
      "0  338118 2015-01-01 NaN            NaN       NaN        NaN        NaN   \n",
      "1  338118 2015-01-02 NaN            NaN       NaN        NaN        NaN   \n",
      "2  338118 2015-01-03 NaN            NaN       NaN        NaN        NaN   \n",
      "3  338118 2015-01-04 NaN            NaN       NaN        NaN        NaN   \n",
      "4  338118 2015-01-05 NaN            NaN       NaN        NaN        NaN   \n",
      "\n",
      "   X ICL+ICLT pF  X ICL pF  X ICLT pF  X ICLH pF  Y ICT+ICTH pF  Y ICT pF  \\\n",
      "0            NaN       NaN        NaN        NaN            NaN       NaN   \n",
      "1            NaN       NaN        NaN        NaN            NaN       NaN   \n",
      "2            NaN       NaN        NaN        NaN            NaN       NaN   \n",
      "3            NaN       NaN        NaN        NaN            NaN       NaN   \n",
      "4            NaN       NaN        NaN        NaN            NaN       NaN   \n",
      "\n",
      "   Y ICTH pF  Y ICTL pF  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN   \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FURANOS.xlsx\n",
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  FUR\n",
      "0    146660T3 2022-05-12  1.0\n",
      "1    146660T3 2020-06-13  1.0\n",
      "2    146660T3 2019-01-25  1.0\n",
      "3    146660T3 2018-08-09  1.0\n",
      "4      364076 2022-06-18  1.0\n",
      "5      364076 2020-06-13  1.0\n",
      "6      364076 2016-01-21  1.0\n",
      "7      364076 2015-05-02  2.0\n",
      "8   230531-01 2024-07-16  2.0\n",
      "9   230531-01 2020-05-12  2.0\n",
      "10  230531-01 2018-07-05  2.0\n",
      "11  230531-01 2015-04-26  2.0\n",
      "12     338118 2024-11-13  2.0\n",
      "13     338118 2019-10-28  2.0\n",
      "14     338118 2018-03-17  2.0\n",
      "15     338118 2015-11-14  2.0 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "           SERIE      FECHA  FUR\n",
      "0         338118 2015-01-01  NaN\n",
      "1         338118 2015-01-02  NaN\n",
      "2         338118 2015-01-03  NaN\n",
      "3         338118 2015-01-04  NaN\n",
      "4         338118 2015-01-05  NaN\n",
      "...          ...        ...  ...\n",
      "15723  230531-01 2025-10-02  2.0\n",
      "15724  230531-01 2025-10-03  2.0\n",
      "15725  230531-01 2025-10-04  2.0\n",
      "15726  230531-01 2025-10-05  2.0\n",
      "15727  230531-01 2025-10-06  2.0\n",
      "\n",
      "[15728 rows x 3 columns] \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES DE FUR CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  2-Furfuraldehido (FAL, ppb)  FUR\n",
      "0    146660T3 2022-05-12                            1  1.0\n",
      "1    146660T3 2020-06-13                            1  1.0\n",
      "2    146660T3 2019-01-25                            1  1.0\n",
      "3    146660T3 2018-08-09                            1  1.0\n",
      "4      364076 2022-06-18                            1  1.0\n",
      "5      364076 2020-06-13                            2  1.0\n",
      "6      364076 2016-01-21                            2  1.0\n",
      "7      364076 2015-05-02                          143  2.0\n",
      "8   230531-01 2024-07-16                          160  2.0\n",
      "9   230531-01 2020-05-12                          207  2.0\n",
      "10  230531-01 2018-07-05                          319  2.0\n",
      "11  230531-01 2015-04-26                          263  2.0\n",
      "12     338118 2024-11-13                          132  2.0\n",
      "13     338118 2019-10-28                          241  2.0\n",
      "14     338118 2018-03-17                          388  2.0\n",
      "15     338118 2015-11-14                          335  2.0 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES DE FUR CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "           SERIE      FECHA  2-Furfuraldehido (FAL, ppb)  FUR\n",
      "0         338118 2015-01-01                          NaN  NaN\n",
      "1         338118 2015-01-02                          NaN  NaN\n",
      "2         338118 2015-01-03                          NaN  NaN\n",
      "3         338118 2015-01-04                          NaN  NaN\n",
      "4         338118 2015-01-05                          NaN  NaN\n",
      "...          ...        ...                          ...  ...\n",
      "15723  230531-01 2025-10-02                        160.0  2.0\n",
      "15724  230531-01 2025-10-03                        160.0  2.0\n",
      "15725  230531-01 2025-10-04                        160.0  2.0\n",
      "15726  230531-01 2025-10-05                        160.0  2.0\n",
      "15727  230531-01 2025-10-06                        160.0  2.0\n",
      "\n",
      "[15728 rows x 4 columns] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fp['Max_FP'] = df_fp[[c for c in df_fp.columns if c.endswith('%')]].max(axis=1)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[columna_puntaje] = np.select(\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:81: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_FP = df_extendida_FP.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:81: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_FP = df_extendida_FP.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:89: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_FP = df_detalles_ext_FP.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FP.py:89: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_FP = df_detalles_ext_FP.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_cd.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:93: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_CD = df_extendida_CD.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:93: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_CD = df_extendida_CD.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_CD = df_detalles_ext_CD.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CD.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_CD = df_detalles_ext_CD.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FURANOS.py:68: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_FUR = df_extendida_FUR.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FURANOS.py:68: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_FUR = df_extendida_FUR.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FURANOS.py:79: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_detalles_FUR = df_extendida_detalles_FUR.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FURANOS.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_detalles_FUR = df_extendida_detalles_FUR.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SERIE      FECHA  AIS  FPDEVANADO   CD  FUR\n",
      "0  338118 2015-11-14  2.0         NaN  NaN  2.0\n",
      "1  338118 2016-09-02  2.6         5.0  1.0  NaN\n",
      "2  338118 2017-12-18  5.0         5.0  5.0  NaN\n",
      "3  338118 2018-03-17  2.0         NaN  NaN  2.0\n",
      "4  338118 2018-08-29  5.0         5.0  5.0  NaN\n",
      "    SERIE      FECHA  AIS  FPDEVANADO  CD  FUR\n",
      "0  338118 2015-01-01  NaN         NaN NaN  NaN\n",
      "1  338118 2015-01-02  NaN         NaN NaN  NaN\n",
      "2  338118 2015-01-03  NaN         NaN NaN  NaN\n",
      "3  338118 2015-01-04  NaN         NaN NaN  NaN\n",
      "4  338118 2015-01-05  NaN         NaN NaN  NaN\n",
      "           SERIE      FECHA       AIS\n",
      "0         338118 2015-01-01       NaN\n",
      "1         338118 2015-01-02       NaN\n",
      "2         338118 2015-01-03       NaN\n",
      "3         338118 2015-01-04       NaN\n",
      "4         338118 2015-01-05       NaN\n",
      "...          ...        ...       ...\n",
      "15723  230531-01 2025-10-02  4.142857\n",
      "15724  230531-01 2025-10-03  4.142857\n",
      "15725  230531-01 2025-10-04  4.142857\n",
      "15726  230531-01 2025-10-05  4.142857\n",
      "15727  230531-01 2025-10-06  4.142857\n",
      "\n",
      "[15728 rows x 3 columns]\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/RNUCLEO.xlsx\n",
      "        SERIE      FECHA  RNUC  Valor (MΩ)\n",
      "0    146660T3 2022-05-12     1         665\n",
      "1    146660T3 2019-01-23     1         551\n",
      "2    146660T3 2018-02-05     1         933\n",
      "3    146660T3 2017-11-22     1         535\n",
      "4      364076 2022-06-18     1         552\n",
      "5      364076 2015-05-24     1         912\n",
      "6      364076 2014-08-13     1         928\n",
      "7      364076 2013-11-05     5         287\n",
      "8   230531-01 2024-07-16     1         618\n",
      "9   230531-01 2017-10-26     1         978\n",
      "10  230531-01 2016-03-04     5         195\n",
      "11  230531-01 2014-11-11     5         216\n",
      "12     338118 2024-11-13     1         801\n",
      "13     338118 2017-12-13     1         710\n",
      "14     338118 2016-09-05     1         916\n",
      "15     338118 2014-05-28     5         469\n",
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  RNUC\n",
      "0    146660T3 2022-05-12     1\n",
      "1    146660T3 2019-01-23     1\n",
      "2    146660T3 2018-02-05     1\n",
      "3    146660T3 2017-11-22     1\n",
      "4      364076 2022-06-18     1\n",
      "5      364076 2015-05-24     1\n",
      "6      364076 2014-08-13     1\n",
      "7      364076 2013-11-05     5\n",
      "8   230531-01 2024-07-16     1\n",
      "9   230531-01 2017-10-26     1\n",
      "10  230531-01 2016-03-04     5\n",
      "11  230531-01 2014-11-11     5\n",
      "12     338118 2024-11-13     1\n",
      "13     338118 2017-12-13     1\n",
      "14     338118 2016-09-05     1\n",
      "15     338118 2014-05-28     5 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RNUC\n",
      "0  338118 2015-01-01   5.0\n",
      "1  338118 2015-01-02   5.0\n",
      "2  338118 2015-01-03   5.0\n",
      "3  338118 2015-01-04   5.0\n",
      "4  338118 2015-01-05   5.0 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  RNUC  Valor (MΩ)\n",
      "0  146660T3 2022-05-12     1         665\n",
      "1  146660T3 2019-01-23     1         551\n",
      "2  146660T3 2018-02-05     1         933\n",
      "3  146660T3 2017-11-22     1         535\n",
      "4    364076 2022-06-18     1         552 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  RNUC  Valor (MΩ)\n",
      "0  338118 2015-01-01   5.0       469.0\n",
      "1  338118 2015-01-02   5.0       469.0\n",
      "2  338118 2015-01-03   5.0       469.0\n",
      "3  338118 2015-01-04   5.0       469.0\n",
      "4  338118 2015-01-05   5.0       469.0 \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/IEXCITACION.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCrnuc.py:52: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_RNUC_ext = df_RNUC_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCrnuc.py:52: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_RNUC_ext = df_RNUC_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCrnuc.py:64: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCrnuc.py:64: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:30: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df.groupby('SERIE').apply(lambda g: g.loc[g['FECHA'].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:71: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_IEX_ext = df_IEX_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:71: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_IEX_ext = df_IEX_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:83: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\NUCiex.py:83: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "        SERIE      FECHA  IEX\n",
      "0    146660T3 2022-05-12    5\n",
      "1    146660T3 2018-02-16    5\n",
      "2    146660T3 2017-04-15    5\n",
      "3    146660T3 2016-09-24    1\n",
      "4      364076 2022-06-18    5\n",
      "5      364076 2019-04-24    5\n",
      "6      364076 2018-03-07    5\n",
      "7      364076 2014-12-08    1\n",
      "8   230531-01 2024-07-16    5\n",
      "9   230531-01 2019-11-26    5\n",
      "10  230531-01 2018-06-12    5\n",
      "11  230531-01 2015-04-25    1\n",
      "12     338118 2024-11-13    5\n",
      "13     338118 2019-02-13    5\n",
      "14     338118 2017-10-31    5\n",
      "15     338118 2015-04-28    1 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  IEX\n",
      "0  338118 2015-01-01  NaN\n",
      "1  338118 2015-01-02  NaN\n",
      "2  338118 2015-01-03  NaN\n",
      "3  338118 2015-01-04  NaN\n",
      "4  338118 2015-01-05  NaN \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  IEX  R POS1 mA  R POS2 mA  R POS3 mA  R POS4 mA  \\\n",
      "0  146660T3 2022-05-12    5    111.235    191.629    146.622    144.914   \n",
      "1  146660T3 2018-02-16    5    188.693    114.216    133.965    194.859   \n",
      "2  146660T3 2017-04-15    5    100.873    100.875    110.410    124.139   \n",
      "3  146660T3 2016-09-24    1    128.112    143.145    103.937    109.001   \n",
      "4    364076 2022-06-18    5    142.290    180.712    173.369    194.856   \n",
      "\n",
      "   R POS5 mA  R POS6 mA  R POS7 mA  ...  T POS18 mA  T POS19 mA  T POS20 mA  \\\n",
      "0    179.458    121.242    188.996  ...     186.193     160.812     120.611   \n",
      "1    160.695    145.281    110.364  ...     152.812     194.145     129.906   \n",
      "2    189.568    145.275    110.087  ...     155.308     115.643     121.963   \n",
      "3    169.662    195.572    117.368  ...     179.454     101.771     155.218   \n",
      "4    119.309    142.324    199.955  ...     163.629     121.602     137.777   \n",
      "\n",
      "   T POS21 mA  T POS22 mA  T POS23 mA  T POS24 mA  T POS25 mA  T POS26 mA  \\\n",
      "0     114.643     143.999     132.651     176.578     161.892     147.003   \n",
      "1     163.382     188.661     107.555     156.040     119.796     104.434   \n",
      "2     182.579     143.042     164.656     102.757     142.567     173.857   \n",
      "3     146.779     164.411     111.809     188.801     148.031     146.680   \n",
      "4     198.430     153.521     186.329     136.476     145.228     190.737   \n",
      "\n",
      "   T POS27 mA  \n",
      "0     124.753  \n",
      "1     170.016  \n",
      "2     118.402  \n",
      "3     110.143  \n",
      "4     160.036  \n",
      "\n",
      "[5 rows x 84 columns] \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  IEX  R POS1 mA  R POS2 mA  R POS3 mA  R POS4 mA  \\\n",
      "0  338118 2015-01-01  NaN        NaN        NaN        NaN        NaN   \n",
      "1  338118 2015-01-02  NaN        NaN        NaN        NaN        NaN   \n",
      "2  338118 2015-01-03  NaN        NaN        NaN        NaN        NaN   \n",
      "3  338118 2015-01-04  NaN        NaN        NaN        NaN        NaN   \n",
      "4  338118 2015-01-05  NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "   R POS5 mA  R POS6 mA  R POS7 mA  ...  T POS18 mA  T POS19 mA  T POS20 mA  \\\n",
      "0        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
      "1        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
      "2        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
      "3        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
      "4        NaN        NaN        NaN  ...         NaN         NaN         NaN   \n",
      "\n",
      "   T POS21 mA  T POS22 mA  T POS23 mA  T POS24 mA  T POS25 mA  T POS26 mA  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   T POS27 mA  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "\n",
      "[5 rows x 84 columns] \n",
      "\n",
      "    SERIE      FECHA  NUC  RNUC  IEX\n",
      "0  338118 2014-05-28  5.0   5.0  NaN\n",
      "1  338118 2015-04-28  1.0   NaN  1.0\n",
      "2  338118 2016-09-05  1.0   1.0  NaN\n",
      "3  338118 2017-10-31  5.0   NaN  5.0\n",
      "4  338118 2017-12-13  1.0   1.0  NaN\n",
      "           SERIE      FECHA  NUC  RNUC  IEX\n",
      "0         338118 2015-01-01  5.0   5.0  NaN\n",
      "1         338118 2015-01-02  5.0   5.0  NaN\n",
      "2         338118 2015-01-03  5.0   5.0  NaN\n",
      "3         338118 2015-01-04  5.0   5.0  NaN\n",
      "4         338118 2015-01-05  5.0   5.0  NaN\n",
      "...          ...        ...  ...   ...  ...\n",
      "15723  230531-01 2025-10-02  3.5   1.0  5.0\n",
      "15724  230531-01 2025-10-03  3.5   1.0  5.0\n",
      "15725  230531-01 2025-10-04  3.5   1.0  5.0\n",
      "15726  230531-01 2025-10-05  3.5   1.0  5.0\n",
      "15727  230531-01 2025-10-06  3.5   1.0  5.0\n",
      "\n",
      "[15728 rows x 5 columns]\n",
      "           SERIE      FECHA  NUC\n",
      "0         338118 2015-01-01  5.0\n",
      "1         338118 2015-01-02  5.0\n",
      "2         338118 2015-01-03  5.0\n",
      "3         338118 2015-01-04  5.0\n",
      "4         338118 2015-01-05  5.0\n",
      "...          ...        ...  ...\n",
      "15723  230531-01 2025-10-02  3.5\n",
      "15724  230531-01 2025-10-03  3.5\n",
      "15725  230531-01 2025-10-04  3.5\n",
      "15726  230531-01 2025-10-05  3.5\n",
      "15727  230531-01 2025-10-06  3.5\n",
      "\n",
      "[15728 rows x 3 columns]\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FQOLTC.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\OLTC.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_OLTC_ext = df_OLTC_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\OLTC.py:54: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_OLTC_ext = df_OLTC_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\OLTC.py:68: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\OLTC.py:68: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext = df_detalles_ext.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC1.py:89: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_FP_C1 = df_extendida_FP_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC1.py:89: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_FP_C1 = df_extendida_FP_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC1.py:97: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_FP_C1 = df_detalles_ext_FP_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC1.py:97: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_FP_C1 = df_detalles_ext_FP_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC2.py:88: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_FP_C2 = df_extendida_FP_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC2.py:88: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_FP_C2 = df_extendida_FP_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC2.py:96: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_FP_C2 = df_detalles_ext_FP_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPBC2.py:96: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_FP_C2 = df_detalles_ext_FP_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c1.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c1.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c1.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c1.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:103: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_C_C1 = df_extendida_C_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:103: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_C_C1 = df_extendida_C_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:111: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_C_C1 = df_detalles_ext_C_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC1.py:111: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_C_C1 = df_detalles_ext_C_C1.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c2.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c2.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c2.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = df_c_c2.groupby(\"SERIE\").apply(lambda g: g.loc[g[\"FECHA\"].idxmin(), col])\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:102: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_C_C2 = df_extendida_C_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:102: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_C_C2 = df_extendida_C_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:110: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_C_C2 = df_detalles_ext_C_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\CBC2.py:110: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_C_C2 = df_detalles_ext_C_C2.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPCOCA.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_extendida_CC = df_extendida_CC.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPCOCA.py:77: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_extendida_CC = df_extendida_CC.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPCOCA.py:85: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_detalles_ext_CC = df_detalles_ext_CC.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n",
      "c:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\\FPCOCA.py:85: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_detalles_ext_CC = df_detalles_ext_CC.groupby(\"SERIE\").apply(lambda g: g.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====== TABLA CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  OLTC\n",
      "0  146660T3 2022-05-12   5.0\n",
      "1  146660T3 2018-05-12   1.0\n",
      "2  146660T3 2016-12-30   1.0\n",
      "3  146660T3 2014-03-26   2.5\n",
      "4    364076 2022-06-18   2.5 \n",
      "\n",
      "\n",
      " ====== TABLA CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "    SERIE      FECHA  OLTC\n",
      "0  338118 2015-01-01   NaN\n",
      "1  338118 2015-01-02   NaN\n",
      "2  338118 2015-01-03   NaN\n",
      "3  338118 2015-01-04   NaN\n",
      "4  338118 2015-01-05   NaN \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS ORIGINALES ====== \n",
      "\n",
      "      SERIE      FECHA  OLTC  RD  H20\n",
      "0  146660T3 2022-05-12   5.0  18   48\n",
      "1  146660T3 2018-05-12   1.0  33   12\n",
      "2  146660T3 2016-12-30   1.0  34   14\n",
      "3  146660T3 2014-03-26   2.5  48   46\n",
      "4    364076 2022-06-18   2.5  58   33 \n",
      "\n",
      "\n",
      " ====== TABLA DE DETALLES CON FECHAS EXTENDIDAS ====== \n",
      "\n",
      "           SERIE      FECHA  OLTC    RD   H20\n",
      "0         338118 2015-01-01   NaN   NaN   NaN\n",
      "1         338118 2015-01-02   NaN   NaN   NaN\n",
      "2         338118 2015-01-03   NaN   NaN   NaN\n",
      "3         338118 2015-01-04   NaN   NaN   NaN\n",
      "4         338118 2015-01-05   NaN   NaN   NaN\n",
      "...          ...        ...   ...   ...   ...\n",
      "15723  230531-01 2025-10-02   3.5  28.0  19.0\n",
      "15724  230531-01 2025-10-03   3.5  28.0  19.0\n",
      "15725  230531-01 2025-10-04   3.5  28.0  19.0\n",
      "15726  230531-01 2025-10-05   3.5  28.0  19.0\n",
      "15727  230531-01 2025-10-06   3.5  28.0  19.0\n",
      "\n",
      "[15728 rows x 5 columns] \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPC1C2.xlsx\n",
      "\n",
      " ====== TABLA FPBC1 ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA  FPBC1\n",
      "11  230531-01 2015-02-11    5.0\n",
      "12     338118 2024-11-13    5.0\n",
      "13     338118 2022-08-19    5.0\n",
      "14     338118 2016-01-06    5.0\n",
      "15     338118 2015-09-22    5.0 \n",
      "\n",
      "\n",
      " ====== TABLA FPBC1 EXTENDIDA ====== \n",
      "\n",
      "           SERIE      FECHA  FPBC1\n",
      "15723  230531-01 2025-10-02    5.0\n",
      "15724  230531-01 2025-10-03    5.0\n",
      "15725  230531-01 2025-10-04    5.0\n",
      "15726  230531-01 2025-10-05    5.0\n",
      "15727  230531-01 2025-10-06    5.0 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA FPBC1 ====== \n",
      "\n",
      "      SERIE      FECHA  FPBC1  C1 H1 %  C1 H2 %  C1 H3 %  C1 H0 %\n",
      "0  146660T3 2022-05-12    3.0     0.92     0.43     0.11     0.15\n",
      "1  146660T3 2019-04-27    5.0     0.76     0.51     1.92     0.12\n",
      "2  146660T3 2015-05-15    5.0     0.23     1.09     1.40     0.43\n",
      "3  146660T3 2014-11-14    5.0     0.94     1.68     1.89     1.42\n",
      "4    364076 2022-06-18    5.0     0.32     0.72     0.59     1.87 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA FPBC1 ====== \n",
      "\n",
      "           SERIE      FECHA  FPBC1  C1 H1 %  C1 H2 %  C1 H3 %  C1 H0 %\n",
      "15723  230531-01 2025-10-02    5.0     0.66     1.88     1.37     1.04\n",
      "15724  230531-01 2025-10-03    5.0     0.66     1.88     1.37     1.04\n",
      "15725  230531-01 2025-10-04    5.0     0.66     1.88     1.37     1.04\n",
      "15726  230531-01 2025-10-05    5.0     0.66     1.88     1.37     1.04\n",
      "15727  230531-01 2025-10-06    5.0     0.66     1.88     1.37     1.04 \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPC1C2.xlsx\n",
      "\n",
      " ====== TABLA FPBC2 ORIGINAL ====== \n",
      "\n",
      "      SERIE      FECHA  FPBC2\n",
      "0  146660T3 2022-05-12    5.0\n",
      "1  146660T3 2019-04-27    5.0\n",
      "2  146660T3 2015-05-15    5.0\n",
      "3  146660T3 2014-11-14    5.0\n",
      "4    364076 2022-06-18    5.0 \n",
      "\n",
      "\n",
      " ====== TABLA FPBC2 EXTENDIDA ====== \n",
      "\n",
      "           SERIE      FECHA  FPBC2\n",
      "15723  230531-01 2025-10-02    5.0\n",
      "15724  230531-01 2025-10-03    5.0\n",
      "15725  230531-01 2025-10-04    5.0\n",
      "15726  230531-01 2025-10-05    5.0\n",
      "15727  230531-01 2025-10-06    5.0 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA FPBC2 ====== \n",
      "\n",
      "      SERIE      FECHA  FPBC2  C2 H1 %  C2 H2 %  C2 H3 %  C2 H0 %\n",
      "0  146660T3 2022-05-12    5.0     6.65     9.90     4.73     9.83\n",
      "1  146660T3 2019-04-27    5.0     4.77     9.65     4.78     5.24\n",
      "2  146660T3 2015-05-15    5.0     3.32     0.46     2.60     8.02\n",
      "3  146660T3 2014-11-14    5.0     8.06     4.02     9.75     3.96\n",
      "4    364076 2022-06-18    5.0     1.87     8.05     2.53     1.49 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA FPBC2 ====== \n",
      "\n",
      "           SERIE      FECHA  FPBC2  C2 H1 %  C2 H2 %  C2 H3 %  C2 H0 %\n",
      "15723  230531-01 2025-10-02    5.0     9.23     8.83     5.49     3.21\n",
      "15724  230531-01 2025-10-03    5.0     9.23     8.83     5.49     3.21\n",
      "15725  230531-01 2025-10-04    5.0     9.23     8.83     5.49     3.21\n",
      "15726  230531-01 2025-10-05    5.0     9.23     8.83     5.49     3.21\n",
      "15727  230531-01 2025-10-06    5.0     9.23     8.83     5.49     3.21 \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPC1C2.xlsx\n",
      "\n",
      " ====== TABLA CBC1 ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA  CBC1\n",
      "0    146660T3 2022-05-12   5.0\n",
      "1    146660T3 2019-04-27   5.0\n",
      "2    146660T3 2015-05-15   5.0\n",
      "3    146660T3 2014-11-14   1.0\n",
      "4      364076 2022-06-18   5.0\n",
      "5      364076 2019-06-27   5.0\n",
      "6      364076 2017-02-20   5.0\n",
      "7      364076 2016-12-30   1.0\n",
      "8   230531-01 2024-07-16   5.0\n",
      "9   230531-01 2022-06-26   5.0\n",
      "10  230531-01 2020-12-14   5.0\n",
      "11  230531-01 2015-02-11   1.0\n",
      "12     338118 2024-11-13   5.0\n",
      "13     338118 2022-08-19   5.0\n",
      "14     338118 2016-01-06   5.0\n",
      "15     338118 2015-09-22   1.0 \n",
      "\n",
      "\n",
      " ====== TABLA CBC1 EXTENDIDA ====== \n",
      "\n",
      "    SERIE      FECHA  CBC1\n",
      "0  338118 2015-01-01   NaN\n",
      "1  338118 2015-01-02   NaN\n",
      "2  338118 2015-01-03   NaN\n",
      "3  338118 2015-01-04   NaN\n",
      "4  338118 2015-01-05   NaN \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA CBC1 ====== \n",
      "\n",
      "      SERIE      FECHA  CBC1  C1 H1 pF  C1 H2 pF  C1 H3 pF  C1 H0 pF\n",
      "0  146660T3 2022-05-12   5.0      4943      4295      1709       246\n",
      "1  146660T3 2019-04-27   5.0      2163       807      3261      4874\n",
      "2  146660T3 2015-05-15   5.0      2215      2681      3458      4995\n",
      "3  146660T3 2014-11-14   1.0       246      3372      1718      2143\n",
      "4    364076 2022-06-18   5.0      3758      4644      4239       502 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA CBC1 ====== \n",
      "\n",
      "    SERIE      FECHA  CBC1  C1 H1 pF  C1 H2 pF  C1 H3 pF  C1 H0 pF\n",
      "0  338118 2015-01-01   NaN       NaN       NaN       NaN       NaN\n",
      "1  338118 2015-01-02   NaN       NaN       NaN       NaN       NaN\n",
      "2  338118 2015-01-03   NaN       NaN       NaN       NaN       NaN\n",
      "3  338118 2015-01-04   NaN       NaN       NaN       NaN       NaN\n",
      "4  338118 2015-01-05   NaN       NaN       NaN       NaN       NaN \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPC1C2.xlsx\n",
      "\n",
      " ====== TABLA CBC2 ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA  CBC2\n",
      "0    146660T3 2022-05-12   5.0\n",
      "1    146660T3 2019-04-27   5.0\n",
      "2    146660T3 2015-05-15   5.0\n",
      "3    146660T3 2014-11-14   1.0\n",
      "4      364076 2022-06-18   5.0\n",
      "5      364076 2019-06-27   5.0\n",
      "6      364076 2017-02-20   5.0\n",
      "7      364076 2016-12-30   1.0\n",
      "8   230531-01 2024-07-16   5.0\n",
      "9   230531-01 2022-06-26   5.0\n",
      "10  230531-01 2020-12-14   5.0\n",
      "11  230531-01 2015-02-11   1.0\n",
      "12     338118 2024-11-13   5.0\n",
      "13     338118 2022-08-19   5.0\n",
      "14     338118 2016-01-06   5.0\n",
      "15     338118 2015-09-22   1.0 \n",
      "\n",
      "\n",
      " ====== TABLA CBC2 EXTENDIDA ====== \n",
      "\n",
      "    SERIE      FECHA  CBC2\n",
      "0  338118 2015-01-01   NaN\n",
      "1  338118 2015-01-02   NaN\n",
      "2  338118 2015-01-03   NaN\n",
      "3  338118 2015-01-04   NaN\n",
      "4  338118 2015-01-05   NaN \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA CBC2 ====== \n",
      "\n",
      "      SERIE      FECHA  CBC2  C2 H1 pF  C2 H2 pF  C2 H3 pF  C2 NH0 pF\n",
      "0  146660T3 2022-05-12   5.0      3779      1236      3863       3385\n",
      "1  146660T3 2019-04-27   5.0      4059      3342      1351        715\n",
      "2  146660T3 2015-05-15   5.0      3224      1120      4255       3712\n",
      "3  146660T3 2014-11-14   1.0      3471      1069       198       3445\n",
      "4    364076 2022-06-18   5.0      4343      3495      2393       2038 \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA CBC2 ====== \n",
      "\n",
      "    SERIE      FECHA  CBC2  C2 H1 pF  C2 H2 pF  C2 H3 pF  C2 NH0 pF\n",
      "0  338118 2015-01-01   NaN       NaN       NaN       NaN        NaN\n",
      "1  338118 2015-01-02   NaN       NaN       NaN       NaN        NaN\n",
      "2  338118 2015-01-03   NaN       NaN       NaN       NaN        NaN\n",
      "3  338118 2015-01-04   NaN       NaN       NaN       NaN        NaN\n",
      "4  338118 2015-01-05   NaN       NaN       NaN       NaN        NaN \n",
      "\n",
      "✅ Archivo cargado desde: C:/Users/roquispec/OneDrive - LUZ DEL SUR S.A.A/Documentos/Estudios de Ingreso/ProyectoRyD_V2/Basededatos/FPCOCA.xlsx\n",
      "\n",
      " ====== TABLA CC ORIGINAL ====== \n",
      "\n",
      "        SERIE      FECHA   CC\n",
      "0    146660T3 2022-05-12  5.0\n",
      "1    146660T3 2019-04-27  5.0\n",
      "2    146660T3 2015-05-15  5.0\n",
      "3    146660T3 2014-11-14  5.0\n",
      "4      364076 2022-06-18  5.0\n",
      "5      364076 2019-06-27  5.0\n",
      "6      364076 2017-02-20  5.0\n",
      "7      364076 2016-12-30  5.0\n",
      "8   230531-01 2024-07-16  5.0\n",
      "9   230531-01 2022-06-26  5.0\n",
      "10  230531-01 2020-12-14  5.0\n",
      "11  230531-01 2015-02-11  5.0\n",
      "12     338118 2024-11-13  5.0\n",
      "13     338118 2022-08-19  5.0\n",
      "14     338118 2016-01-06  5.0\n",
      "15     338118 2015-09-22  5.0 \n",
      "\n",
      "\n",
      " ====== TABLA CC EXTENDIDA ====== \n",
      "\n",
      "    SERIE      FECHA  CC\n",
      "0  338118 2015-01-01 NaN\n",
      "1  338118 2015-01-02 NaN\n",
      "2  338118 2015-01-03 NaN\n",
      "3  338118 2015-01-04 NaN\n",
      "4  338118 2015-01-05 NaN \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA CC ====== \n",
      "\n",
      "      SERIE      FECHA   CC  X1 mW  X2 mW  X3 mW  X0 mW  Y1 mW  Y2 mW  Y3 mW  \\\n",
      "0  146660T3 2022-05-12  5.0    112    857    872    251    796    801    602   \n",
      "1  146660T3 2019-04-27  5.0    337    164    703    613    855    857    735   \n",
      "2  146660T3 2015-05-15  5.0    476    360    108    904    821    510    434   \n",
      "3  146660T3 2014-11-14  5.0    593    713    482    307    642    463     75   \n",
      "4    364076 2022-06-18  5.0    121    731    554    605     97    211    589   \n",
      "\n",
      "   Y0 mW  \n",
      "0    645  \n",
      "1    850  \n",
      "2    220  \n",
      "3    390  \n",
      "4     53   \n",
      "\n",
      "\n",
      " ====== TABLA DETALLADA EXTENDIDA CC ====== \n",
      "\n",
      "    SERIE      FECHA  CC  X1 mW  X2 mW  X3 mW  X0 mW  Y1 mW  Y2 mW  Y3 mW  \\\n",
      "0  338118 2015-01-01 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "1  338118 2015-01-02 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "2  338118 2015-01-03 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "3  338118 2015-01-04 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "4  338118 2015-01-05 NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
      "\n",
      "   Y0 mW  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN   \n",
      "\n",
      "           SERIE      FECHA  BUS\n",
      "0         338118 2015-01-01  NaN\n",
      "1         338118 2015-01-02  NaN\n",
      "2         338118 2015-01-03  NaN\n",
      "3         338118 2015-01-04  NaN\n",
      "4         338118 2015-01-05  NaN\n",
      "...          ...        ...  ...\n",
      "15723  230531-01 2025-10-02  5.0\n",
      "15724  230531-01 2025-10-03  5.0\n",
      "15725  230531-01 2025-10-04  5.0\n",
      "15726  230531-01 2025-10-05  5.0\n",
      "15727  230531-01 2025-10-06  5.0\n",
      "\n",
      "[15728 rows x 3 columns]\n",
      "[DGA] columnas: ['SERIE', 'FECHA', 'DGA']\n",
      "[ACE] columnas: ['SERIE', 'FECHA', 'ACE']\n",
      "[ARR] columnas: ['SERIE', 'FECHA', 'ARR']\n",
      "[AIS] columnas: ['SERIE', 'FECHA', 'AIS']\n",
      "[NUC] columnas: ['SERIE', 'FECHA', 'NUC']\n",
      "[OLTC] columnas: ['SERIE', 'FECHA', 'OLTC']\n",
      "[BUS] columnas: ['SERIE', 'FECHA', 'BUS']\n",
      "           SERIE      FECHA        HI  DGA       ACE  ARR       AIS  NUC  \\\n",
      "0         338118 2015-01-01  3.451049  1.3  4.181818  5.0       NaN  5.0   \n",
      "1         338118 2015-01-02  3.451049  1.3  4.181818  5.0       NaN  5.0   \n",
      "2         338118 2015-01-03  3.451049  1.3  4.181818  5.0       NaN  5.0   \n",
      "3         338118 2015-01-04  3.451049  1.3  4.181818  5.0       NaN  5.0   \n",
      "4         338118 2015-01-05  3.451049  1.3  4.181818  5.0       NaN  5.0   \n",
      "...          ...        ...       ...  ...       ...  ...       ...  ...   \n",
      "15723  230531-01 2025-10-02  3.910714  2.8  4.000000  5.0  4.142857  3.5   \n",
      "15724  230531-01 2025-10-03  3.910714  2.8  4.000000  5.0  4.142857  3.5   \n",
      "15725  230531-01 2025-10-04  3.910714  2.8  4.000000  5.0  4.142857  3.5   \n",
      "15726  230531-01 2025-10-05  3.910714  2.8  4.000000  5.0  4.142857  3.5   \n",
      "15727  230531-01 2025-10-06  3.910714  2.8  4.000000  5.0  4.142857  3.5   \n",
      "\n",
      "       OLTC  BUS  \n",
      "0       NaN  NaN  \n",
      "1       NaN  NaN  \n",
      "2       NaN  NaN  \n",
      "3       NaN  NaN  \n",
      "4       NaN  NaN  \n",
      "...     ...  ...  \n",
      "15723   3.5  5.0  \n",
      "15724   3.5  5.0  \n",
      "15725   3.5  5.0  \n",
      "15726   3.5  5.0  \n",
      "15727   3.5  5.0  \n",
      "\n",
      "[15728 rows x 10 columns]\n",
      "[DGA] columnas: ['SERIE', 'FECHA', 'DGA']\n",
      "[ACE] columnas: ['SERIE', 'FECHA', 'ACE']\n",
      "[ARR] columnas: ['SERIE', 'FECHA', 'ARR']\n",
      "[AIS] columnas: ['SERIE', 'FECHA', 'AIS']\n",
      "[NUC] columnas: ['SERIE', 'FECHA', 'NUC']\n",
      "[OLTC] columnas: ['SERIE', 'FECHA', 'OLTC']\n",
      "[BUS] columnas: ['SERIE', 'FECHA', 'BUS']\n",
      "SERIE            object\n",
      "FECHA    datetime64[ns]\n",
      "HI              float64\n",
      "DGA             float64\n",
      "ACE             float64\n",
      "ARR             float64\n",
      "AIS             float64\n",
      "NUC             float64\n",
      "OLTC            float64\n",
      "BUS             float64\n",
      "dtype: object\n",
      "       SERIE      FECHA        HI   DGA       ACE  ARR       AIS  NUC  OLTC  \\\n",
      "0  230531-01 2025-10-06  3.910714  2.80  4.000000  5.0  4.142857  3.5   3.5   \n",
      "1   146660T3 2025-10-06  3.862347  2.65  4.090909  5.0  3.857143  3.5   5.0   \n",
      "2     338118 2025-10-06  3.584578  2.35  2.363636  5.0  4.142857  3.5   2.5   \n",
      "\n",
      "        BUS  \n",
      "0  5.000000  \n",
      "1  4.529412  \n",
      "2  5.000000  \n",
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'HI', 'DGA', 'ACE', 'ARR', 'AIS', 'NUC', 'OLTC', 'BUS']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME HI:\n",
    "from main import obtener_HI\n",
    "\n",
    "df_HI = obtener_HI()\n",
    "df_HI = df_HI.sort_values(by=[\"FECHA\", \"HI\"], ascending=[False, False]).reset_index(drop=True)\n",
    "print(df_HI.dtypes)\n",
    "print(df_HI.head(3))\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_HI.to_sql('hi_general', engine, if_exists='replace', index=False, schema='processed_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_HI)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_HI.columns))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d324e8",
   "metadata": {},
   "source": [
    "# IMPORTAR hidga detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64602e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'DGA', 'H2', 'CH4', 'C2H2', 'C2H4', 'C2H6', 'CO', 'CO2', 'O2']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME HI:\n",
    "from DGA import get_df_detalles_DGA\n",
    "\n",
    "df_dga = get_df_detalles_DGA()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_dga.to_sql('hi_dga', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_dga)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_dga.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1044df",
   "metadata": {},
   "source": [
    "# IMPORTAR hidga detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caf6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'DGA', 'H2', 'CH4', 'C2H2', 'C2H4', 'C2H6', 'CO', 'CO2', 'O2']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME HI:\n",
    "from DGA import get_df_detalles_ext_DGA\n",
    "\n",
    "df_dga_ext = get_df_detalles_ext_DGA()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_dga_ext.to_sql('hi_dga_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_dga_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_dga_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb70212",
   "metadata": {},
   "source": [
    "# IMPORTAR hiace detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef55482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ACE', 'FP25', 'FP100', 'HU', 'AC', 'TIF', 'CO', 'RD', 'IO']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ACE import get_df_detalles_ACE\n",
    "\n",
    "df_ace = get_df_detalles_ACE()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ace.to_sql('hi_ace', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ace)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ace.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29e890",
   "metadata": {},
   "source": [
    "# IMPORTAR hiace detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b8c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ACE', 'FP25', 'FP100', 'HU', 'AC', 'TIF', 'CO', 'RD', 'IO']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ACE import get_df_detalles_ext_ACE\n",
    "\n",
    "df_ace_ext = get_df_detalles_ext_ACE()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ace_ext.to_sql('hi_ace_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ace_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ace_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b92eb",
   "metadata": {},
   "source": [
    "# IMPORTAR hiarr detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fcb3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 39 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ARR', 'ROHM', 'RTRA', 'RDIS']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARR import get_df_detalles_ARR\n",
    "\n",
    "df_arr = get_df_detalles_ARR()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr.to_sql('hi_arr', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b30da",
   "metadata": {},
   "source": [
    "# IMPORTAR hiarr detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85db5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ARR', 'ROHM', 'RTRA', 'RDIS']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARR import get_df_detalles_ext_ARR\n",
    "\n",
    "df_arr_ext = get_df_detalles_ext_ARR()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_ext.to_sql('hi_arr_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6716ffe",
   "metadata": {},
   "source": [
    "# IMPORTAR rohm detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6773795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ROHM', 'H POS1 R 75°C [mΩ]', 'H POS2 R 75°C [mΩ]', 'H POS3 R 75°C [mΩ]', 'H POS4 R 75°C [mΩ]', 'H POS5 R 75°C [mΩ]', 'H POS6 R 75°C [mΩ]', 'H POS7 R 75°C [mΩ]', 'H POS8 R 75°C [mΩ]', 'H POS9 R 75°C [mΩ]', 'H POS10 R 75°C [mΩ]', 'H POS11 R 75°C [mΩ]', 'H POS12 R 75°C [mΩ]', 'H POS13 R 75°C [mΩ]', 'H POS14 R 75°C [mΩ]', 'H POS15 R 75°C [mΩ]', 'H POS16 R 75°C [mΩ]', 'H POS17 R 75°C [mΩ]', 'H POS18 R 75°C [mΩ]', 'H POS19 R 75°C [mΩ]', 'H POS20 R 75°C [mΩ]', 'H POS21 R 75°C [mΩ]', 'H POS22 R 75°C [mΩ]', 'H POS23 R 75°C [mΩ]', 'H POS24 R 75°C [mΩ]', 'H POS25 R 75°C [mΩ]', 'H POS26 R 75°C [mΩ]', 'H POS27 R 75°C [mΩ]', 'H POS1 S 75°C [mΩ]', 'H POS2 S 75°C [mΩ]', 'H POS3 S 75°C [mΩ]', 'H POS4 S 75°C [mΩ]', 'H POS5 S 75°C [mΩ]', 'H POS6 S 75°C [mΩ]', 'H POS7 S 75°C [mΩ]', 'H POS8 S 75°C [mΩ]', 'H POS9 S 75°C [mΩ]', 'H POS10 S 75°C [mΩ]', 'H POS11 S 75°C [mΩ]', 'H POS12 S 75°C [mΩ]', 'H POS13 S 75°C [mΩ]', 'H POS14 S 75°C [mΩ]', 'H POS15 S 75°C [mΩ]', 'H POS16 S 75°C [mΩ]', 'H POS17 S 75°C [mΩ]', 'H POS18 S 75°C [mΩ]', 'H POS19 S 75°C [mΩ]', 'H POS20 S 75°C [mΩ]', 'H POS21 S 75°C [mΩ]', 'H POS22 S 75°C [mΩ]', 'H POS23 S 75°C [mΩ]', 'H POS24 S 75°C [mΩ]', 'H POS25 S 75°C [mΩ]', 'H POS26 S 75°C [mΩ]', 'H POS27 S 75°C [mΩ]', 'H POS1 T 75°C [mΩ]', 'H POS2 T 75°C [mΩ]', 'H POS3 T 75°C [mΩ]', 'H POS4 T 75°C [mΩ]', 'H POS5 T 75°C [mΩ]', 'H POS6 T 75°C [mΩ]', 'H POS7 T 75°C [mΩ]', 'H POS8 T 75°C [mΩ]', 'H POS9 T 75°C [mΩ]', 'H POS10 T 75°C [mΩ]', 'H POS11 T 75°C [mΩ]', 'H POS12 T 75°C [mΩ]', 'H POS13 T 75°C [mΩ]', 'H POS14 T 75°C [mΩ]', 'H POS15 T 75°C [mΩ]', 'H POS16 T 75°C [mΩ]', 'H POS17 T 75°C [mΩ]', 'H POS18 T 75°C [mΩ]', 'H POS19 T 75°C [mΩ]', 'H POS20 T 75°C [mΩ]', 'H POS21 T 75°C [mΩ]', 'H POS22 T 75°C [mΩ]', 'H POS23 T 75°C [mΩ]', 'H POS24 T 75°C [mΩ]', 'H POS25 T 75°C [mΩ]', 'H POS26 T 75°C [mΩ]', 'H POS27 T 75°C [mΩ]', 'X POS1 R 75°C[mΩ]', 'X POS1 S 75°C[mΩ]', 'X POS1 T 75°C[mΩ]', 'Y POS1 R 75°C[mΩ]', 'Y POS1 S 75°C[mΩ]', 'Y POS1 T 75°C[mΩ]']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARRrohm import get_df_detalles_ROHM\n",
    "\n",
    "df_arr_rohm = get_df_detalles_ROHM()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rohm.to_sql('hi_arr_rohm', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rohm)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rohm.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f7d8f",
   "metadata": {},
   "source": [
    "# IMPORTAR rohm detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc92231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'ROHM', 'H POS1 R 75°C [mΩ]', 'H POS2 R 75°C [mΩ]', 'H POS3 R 75°C [mΩ]', 'H POS4 R 75°C [mΩ]', 'H POS5 R 75°C [mΩ]', 'H POS6 R 75°C [mΩ]', 'H POS7 R 75°C [mΩ]', 'H POS8 R 75°C [mΩ]', 'H POS9 R 75°C [mΩ]', 'H POS10 R 75°C [mΩ]', 'H POS11 R 75°C [mΩ]', 'H POS12 R 75°C [mΩ]', 'H POS13 R 75°C [mΩ]', 'H POS14 R 75°C [mΩ]', 'H POS15 R 75°C [mΩ]', 'H POS16 R 75°C [mΩ]', 'H POS17 R 75°C [mΩ]', 'H POS18 R 75°C [mΩ]', 'H POS19 R 75°C [mΩ]', 'H POS20 R 75°C [mΩ]', 'H POS21 R 75°C [mΩ]', 'H POS22 R 75°C [mΩ]', 'H POS23 R 75°C [mΩ]', 'H POS24 R 75°C [mΩ]', 'H POS25 R 75°C [mΩ]', 'H POS26 R 75°C [mΩ]', 'H POS27 R 75°C [mΩ]', 'H POS1 S 75°C [mΩ]', 'H POS2 S 75°C [mΩ]', 'H POS3 S 75°C [mΩ]', 'H POS4 S 75°C [mΩ]', 'H POS5 S 75°C [mΩ]', 'H POS6 S 75°C [mΩ]', 'H POS7 S 75°C [mΩ]', 'H POS8 S 75°C [mΩ]', 'H POS9 S 75°C [mΩ]', 'H POS10 S 75°C [mΩ]', 'H POS11 S 75°C [mΩ]', 'H POS12 S 75°C [mΩ]', 'H POS13 S 75°C [mΩ]', 'H POS14 S 75°C [mΩ]', 'H POS15 S 75°C [mΩ]', 'H POS16 S 75°C [mΩ]', 'H POS17 S 75°C [mΩ]', 'H POS18 S 75°C [mΩ]', 'H POS19 S 75°C [mΩ]', 'H POS20 S 75°C [mΩ]', 'H POS21 S 75°C [mΩ]', 'H POS22 S 75°C [mΩ]', 'H POS23 S 75°C [mΩ]', 'H POS24 S 75°C [mΩ]', 'H POS25 S 75°C [mΩ]', 'H POS26 S 75°C [mΩ]', 'H POS27 S 75°C [mΩ]', 'H POS1 T 75°C [mΩ]', 'H POS2 T 75°C [mΩ]', 'H POS3 T 75°C [mΩ]', 'H POS4 T 75°C [mΩ]', 'H POS5 T 75°C [mΩ]', 'H POS6 T 75°C [mΩ]', 'H POS7 T 75°C [mΩ]', 'H POS8 T 75°C [mΩ]', 'H POS9 T 75°C [mΩ]', 'H POS10 T 75°C [mΩ]', 'H POS11 T 75°C [mΩ]', 'H POS12 T 75°C [mΩ]', 'H POS13 T 75°C [mΩ]', 'H POS14 T 75°C [mΩ]', 'H POS15 T 75°C [mΩ]', 'H POS16 T 75°C [mΩ]', 'H POS17 T 75°C [mΩ]', 'H POS18 T 75°C [mΩ]', 'H POS19 T 75°C [mΩ]', 'H POS20 T 75°C [mΩ]', 'H POS21 T 75°C [mΩ]', 'H POS22 T 75°C [mΩ]', 'H POS23 T 75°C [mΩ]', 'H POS24 T 75°C [mΩ]', 'H POS25 T 75°C [mΩ]', 'H POS26 T 75°C [mΩ]', 'H POS27 T 75°C [mΩ]', 'X POS1 R 75°C[mΩ]', 'X POS1 S 75°C[mΩ]', 'X POS1 T 75°C[mΩ]', 'Y POS1 R 75°C[mΩ]', 'Y POS1 S 75°C[mΩ]', 'Y POS1 T 75°C[mΩ]']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARRrohm import get_df_detalles_ext_ROHM\n",
    "\n",
    "df_arr_rohm_ext = get_df_detalles_ext_ROHM()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rohm_ext.to_sql('hi_arr_rohm_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rohm_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rohm_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c70682",
   "metadata": {},
   "source": [
    "# IMPORTAR rtra detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171ad9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RTRA', 'H-X POS1 R', 'H-X POS2 R', 'H-X POS3 R', 'H-X POS4 R', 'H-X POS5 R', 'H-X POS6 R', 'H-X POS7 R', 'H-X POS8 R', 'H-X POS9 R', 'H-X POS10 R', 'H-X POS11 R', 'H-X POS12 R', 'H-X POS13 R', 'H-X POS14 R', 'H-X POS15 R', 'H-X POS16 R', 'H-X POS17 R', 'H-X POS18 R', 'H-X POS19 R', 'H-X POS20 R', 'H-X POS21 R', 'H-X POS22 R', 'H-X POS23 R', 'H-X POS24 R', 'H-X POS25 R', 'H-X POS26 R', 'H-X POS27 R', 'H-X POS1 S', 'H-X POS2 S', 'H-X POS3 S', 'H-X POS4 S', 'H-X POS5 S', 'H-X POS6 S', 'H-X POS7 S', 'H-X POS8 S', 'H-X POS9 S', 'H-X POS10 S', 'H-X POS11 S', 'H-X POS12 S', 'H-X POS13 S', 'H-X POS14 S', 'H-X POS15 S', 'H-X POS16 S', 'H-X POS17 S', 'H-X POS18 S', 'H-X POS19 S', 'H-X POS20 S', 'H-X POS21 S', 'H-X POS22 S', 'H-X POS23 S', 'H-X POS24 S', 'H-X POS25 S', 'H-X POS26 S', 'H-X POS27 S', 'H-X POS1 T', 'H-X POS2 T', 'H-X POS3 T', 'H-X POS4 T', 'H-X POS5 T', 'H-X POS6 T', 'H-X POS7 T', 'H-X POS8 T', 'H-X POS9 T', 'H-X POS10 T', 'H-X POS11 T', 'H-X POS12 T', 'H-X POS13 T', 'H-X POS14 T', 'H-X POS15 T', 'H-X POS16 T', 'H-X POS17 T', 'H-X POS18 T', 'H-X POS19 T', 'H-X POS20 T', 'H-X POS21 T', 'H-X POS22 T', 'H-X POS23 T', 'H-X POS24 T', 'H-X POS25 T', 'H-X POS26 T', 'H-X POS27 T', 'H-Y POS1 R', 'H-Y POS2 R', 'H-Y POS3 R', 'H-Y POS4 R', 'H-Y POS5 R', 'H-Y POS6 R', 'H-Y POS7 R', 'H-Y POS8 R', 'H-Y POS9 R', 'H-Y POS10 R', 'H-Y POS11 R', 'H-Y POS12 R', 'H-Y POS13 R', 'H-Y POS14 R', 'H-Y POS15 R', 'H-Y POS16 R', 'H-Y POS17 R', 'H-Y POS18 R', 'H-Y POS19 R', 'H-Y POS20 R', 'H-Y POS21 R', 'H-Y POS22 R', 'H-Y POS23 R', 'H-Y POS24 R', 'H-Y POS25 R', 'H-Y POS26 R', 'H-Y POS27 R', 'H-Y POS1 S', 'H-Y POS2 S', 'H-Y POS3 S', 'H-Y POS4 S', 'H-Y POS5 S', 'H-Y POS6 S', 'H-Y POS7 S', 'H-Y POS8 S', 'H-Y POS9 S', 'H-Y POS10 S', 'H-Y POS11 S', 'H-Y POS12 S', 'H-Y POS13 S', 'H-Y POS14 S', 'H-Y POS15 S', 'H-Y POS16 S', 'H-Y POS17 S', 'H-Y POS18 S', 'H-Y POS19 S', 'H-Y POS20 S', 'H-Y POS21 S', 'H-Y POS22 S', 'H-Y POS23 S', 'H-Y POS24 S', 'H-Y POS25 S', 'H-Y POS26 S', 'H-Y POS27 S', 'H-Y POS1 T', 'H-Y POS2 T', 'H-Y POS3 T', 'H-Y POS4 T', 'H-Y POS5 T', 'H-Y POS6 T', 'H-Y POS7 T', 'H-Y POS8 T', 'H-Y POS9 T', 'H-Y POS10 T', 'H-Y POS11 T', 'H-Y POS12 T', 'H-Y POS13 T', 'H-Y POS14 T', 'H-Y POS15 T', 'H-Y POS16 T', 'H-Y POS17 T', 'H-Y POS18 T', 'H-Y POS19 T', 'H-Y POS20 T', 'H-Y POS21 T', 'H-Y POS22 T', 'H-Y POS23 T', 'H-Y POS24 T', 'H-Y POS25 T', 'H-Y POS26 T', 'H-Y POS27 T', 'X-Y POS1 R', 'X-Y POS1 S', 'X-Y POS1 T']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from ARRrtra import get_df_detalles_RTRA\n",
    "\n",
    "df_arr_rtra = get_df_detalles_RTRA()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rtra.to_sql('hi_arr_rtra', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rtra)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rtra.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa4aaa",
   "metadata": {},
   "source": [
    "# IMPORTAR rtra detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f795c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RTRA', 'H-X POS1 R', 'H-X POS2 R', 'H-X POS3 R', 'H-X POS4 R', 'H-X POS5 R', 'H-X POS6 R', 'H-X POS7 R', 'H-X POS8 R', 'H-X POS9 R', 'H-X POS10 R', 'H-X POS11 R', 'H-X POS12 R', 'H-X POS13 R', 'H-X POS14 R', 'H-X POS15 R', 'H-X POS16 R', 'H-X POS17 R', 'H-X POS18 R', 'H-X POS19 R', 'H-X POS20 R', 'H-X POS21 R', 'H-X POS22 R', 'H-X POS23 R', 'H-X POS24 R', 'H-X POS25 R', 'H-X POS26 R', 'H-X POS27 R', 'H-X POS1 S', 'H-X POS2 S', 'H-X POS3 S', 'H-X POS4 S', 'H-X POS5 S', 'H-X POS6 S', 'H-X POS7 S', 'H-X POS8 S', 'H-X POS9 S', 'H-X POS10 S', 'H-X POS11 S', 'H-X POS12 S', 'H-X POS13 S', 'H-X POS14 S', 'H-X POS15 S', 'H-X POS16 S', 'H-X POS17 S', 'H-X POS18 S', 'H-X POS19 S', 'H-X POS20 S', 'H-X POS21 S', 'H-X POS22 S', 'H-X POS23 S', 'H-X POS24 S', 'H-X POS25 S', 'H-X POS26 S', 'H-X POS27 S', 'H-X POS1 T', 'H-X POS2 T', 'H-X POS3 T', 'H-X POS4 T', 'H-X POS5 T', 'H-X POS6 T', 'H-X POS7 T', 'H-X POS8 T', 'H-X POS9 T', 'H-X POS10 T', 'H-X POS11 T', 'H-X POS12 T', 'H-X POS13 T', 'H-X POS14 T', 'H-X POS15 T', 'H-X POS16 T', 'H-X POS17 T', 'H-X POS18 T', 'H-X POS19 T', 'H-X POS20 T', 'H-X POS21 T', 'H-X POS22 T', 'H-X POS23 T', 'H-X POS24 T', 'H-X POS25 T', 'H-X POS26 T', 'H-X POS27 T', 'H-Y POS1 R', 'H-Y POS2 R', 'H-Y POS3 R', 'H-Y POS4 R', 'H-Y POS5 R', 'H-Y POS6 R', 'H-Y POS7 R', 'H-Y POS8 R', 'H-Y POS9 R', 'H-Y POS10 R', 'H-Y POS11 R', 'H-Y POS12 R', 'H-Y POS13 R', 'H-Y POS14 R', 'H-Y POS15 R', 'H-Y POS16 R', 'H-Y POS17 R', 'H-Y POS18 R', 'H-Y POS19 R', 'H-Y POS20 R', 'H-Y POS21 R', 'H-Y POS22 R', 'H-Y POS23 R', 'H-Y POS24 R', 'H-Y POS25 R', 'H-Y POS26 R', 'H-Y POS27 R', 'H-Y POS1 S', 'H-Y POS2 S', 'H-Y POS3 S', 'H-Y POS4 S', 'H-Y POS5 S', 'H-Y POS6 S', 'H-Y POS7 S', 'H-Y POS8 S', 'H-Y POS9 S', 'H-Y POS10 S', 'H-Y POS11 S', 'H-Y POS12 S', 'H-Y POS13 S', 'H-Y POS14 S', 'H-Y POS15 S', 'H-Y POS16 S', 'H-Y POS17 S', 'H-Y POS18 S', 'H-Y POS19 S', 'H-Y POS20 S', 'H-Y POS21 S', 'H-Y POS22 S', 'H-Y POS23 S', 'H-Y POS24 S', 'H-Y POS25 S', 'H-Y POS26 S', 'H-Y POS27 S', 'H-Y POS1 T', 'H-Y POS2 T', 'H-Y POS3 T', 'H-Y POS4 T', 'H-Y POS5 T', 'H-Y POS6 T', 'H-Y POS7 T', 'H-Y POS8 T', 'H-Y POS9 T', 'H-Y POS10 T', 'H-Y POS11 T', 'H-Y POS12 T', 'H-Y POS13 T', 'H-Y POS14 T', 'H-Y POS15 T', 'H-Y POS16 T', 'H-Y POS17 T', 'H-Y POS18 T', 'H-Y POS19 T', 'H-Y POS20 T', 'H-Y POS21 T', 'H-Y POS22 T', 'H-Y POS23 T', 'H-Y POS24 T', 'H-Y POS25 T', 'H-Y POS26 T', 'H-Y POS27 T', 'X-Y POS1 R', 'X-Y POS1 S', 'X-Y POS1 T']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARRrtra import get_df_detalles_ext_RTRA\n",
    "\n",
    "df_arr_rtra_ext = get_df_detalles_ext_RTRA()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rtra_ext.to_sql('hi_arr_rtra_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rtra_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rtra_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570bcd9",
   "metadata": {},
   "source": [
    "# IMPORTAR rdis detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d95a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RDIS', 'H-X 1 [%] ONAF 2', 'H-X 14 [%] ONAF 2', 'H-X 27 [%] ONAF 2', 'H-Y 1 [%]', 'H-Y 14 [%]', 'H-Y 27 [%]', 'X-Y 1 [%]', 'X-Y 2 [%]', 'X-Y 3 [%]']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from ARRdis import get_df_detalles_DIS\n",
    "\n",
    "df_arr_rdis = get_df_detalles_DIS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rdis.to_sql('hi_arr_rdis', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rdis)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rdis.columns))   \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f989e0",
   "metadata": {},
   "source": [
    "# IMPORTAR rdis detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82466f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RDIS', 'H-X 1 [%] ONAF 2', 'H-X 14 [%] ONAF 2', 'H-X 27 [%] ONAF 2', 'H-Y 1 [%]', 'H-Y 14 [%]', 'H-Y 27 [%]', 'X-Y 1 [%]', 'X-Y 2 [%]', 'X-Y 3 [%]']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from ARRdis import get_df_detalles_ext_DIS\n",
    "\n",
    "df_arr_rdis_ext = get_df_detalles_ext_DIS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_arr_rdis_ext.to_sql('hi_arr_rdis_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_arr_rdis_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_arr_rdis_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0fbc7e",
   "metadata": {},
   "source": [
    "# IMPORTAR hiais detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebe01d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 28 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'AIS', 'FPDEVANADO', 'CD', 'FUR']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from AIS import get_df_detalles_AIS\n",
    "\n",
    "df_ais = get_df_detalles_AIS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais.to_sql('hi_ais', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294050bc",
   "metadata": {},
   "source": [
    "# IMPORTAR hiais detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b55ba925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'AIS', 'FPDEVANADO', 'CD', 'FUR']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from AIS import get_df_detalles_ext_AIS\n",
    "\n",
    "df_ais_ext = get_df_detalles_ext_AIS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_ext.to_sql('hi_ais_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98233cad",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_nuc detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00eba8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 28 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'NUC', 'RNUC', 'IEX']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUC :\n",
    "from NUC import get_df_detalles_NUC\n",
    "\n",
    "df_nuc = get_df_detalles_NUC()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_nuc.to_sql('hi_nuc', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_nuc)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d981efc",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_nuc detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a723d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'NUC', 'RNUC', 'IEX']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUC EXTENDIDO :\n",
    "from NUC import get_df_detalles_ext_NUC\n",
    "\n",
    "df_nuc_ext = get_df_detalles_ext_NUC()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_nuc_ext.to_sql('hi_nuc_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_nuc_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab6102",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_oltc detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6787d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'OLTC', 'RD', 'H20']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME OLTC :\n",
    "from OLTC import df_detalles as df_oltc\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_oltc.to_sql('hi_oltc', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_oltc)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_oltc.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed466794",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_oltc detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f57369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'OLTC', 'RD', 'H20']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME OLTC EXTENDIDO :\n",
    "from OLTC import df_detalles_ext as df_oltc_ext\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_oltc_ext.to_sql('hi_oltc_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_oltc_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_oltc_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118c661",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387fc57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'BUS', 'FPBC1', 'FPBC2', 'CBC1', 'CBC2', 'CC']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME BUS :\n",
    "from BUS import get_df_detalles_BUS\n",
    "from sqlalchemy.types import Date\n",
    "\n",
    "df_bus = get_df_detalles_BUS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_bus.to_sql('hi_bus', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_bus)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2bf06",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eda60e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'BUS', 'FPBC1', 'FPBC2', 'CBC1', 'CBC2', 'CC']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME BUS EXTENDIDO :\n",
    "from BUS import get_df_detalles_ext_BUS\n",
    "\n",
    "df_bus_ext = get_df_detalles_ext_BUS()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_bus_ext.to_sql('hi_bus_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_bus_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b327b6",
   "metadata": {},
   "source": [
    "# IMPORTAR FURANOS detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bcc2669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', '2-Furfuraldehido (FAL, ppb)', 'FUR']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from FURANOS import get_df_detalles_FUR\n",
    "\n",
    "df_ais_furanos = get_df_detalles_FUR()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_furanos.to_sql('hi_ais_furanos', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_furanos)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_furanos.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac57fde",
   "metadata": {},
   "source": [
    "# IMPORTAR FURANOS detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec24df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', '2-Furfuraldehido (FAL, ppb)', 'FUR']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from FURANOS import get_df_detalles_ext_FUR\n",
    "\n",
    "df_ais_furanos_ext = get_df_detalles_ext_FUR()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_furanos_ext.to_sql('hi_ais_furanos_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_furanos_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_furanos_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfecc5d",
   "metadata": {},
   "source": [
    "# IMPORTAR FP detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027f6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'H ICH+ICHL %', 'H ICH %', 'H ICHL %', 'H ICHT %', 'X ICL+ICLT+ICLH %', 'X ICL+ICLT %', 'X ICL %', 'X ICLT %', 'X ICLH %', 'Y ICT+ICTH+ICTL %', 'Y ICT+ICTH %', 'Y ICT %', 'Y ICTH %', 'Y ICTL %', 'FPDEVANADO']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from FP import get_df_detalles_FP\n",
    "\n",
    "df_ais_fp = get_df_detalles_FP()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_fp.to_sql('hi_ais_fp', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_fp)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_fp.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25948a",
   "metadata": {},
   "source": [
    "# IMPORTAR FP detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ef33d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'H ICH+ICHL %', 'H ICH %', 'H ICHL %', 'H ICHT %', 'X ICL+ICLT+ICLH %', 'X ICL+ICLT %', 'X ICL %', 'X ICLT %', 'X ICLH %', 'Y ICT+ICTH+ICTL %', 'Y ICT+ICTH %', 'Y ICT %', 'Y ICTH %', 'Y ICTL %', 'FPDEVANADO']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from FP import get_df_detalles_ext_FP\n",
    "\n",
    "df_ais_fp_ext = get_df_detalles_ext_FP()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_fp_ext.to_sql('hi_ais_fp_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_fp_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_fp_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d72c8b",
   "metadata": {},
   "source": [
    "# IMPORTAR CD Detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea6a66ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CD', 'H ICH+ICHL pF', 'H ICH pF', 'H ICHL pF', 'H ICHT pF', 'X ICL+ICLT pF', 'X ICL pF', 'X ICLT pF', 'X ICLH pF', 'Y ICT+ICTH pF', 'Y ICT pF', 'Y ICTH pF', 'Y ICTL pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ARR :\n",
    "from CD import get_df_detalles_CD\n",
    "\n",
    "df_ais_cd = get_df_detalles_CD()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la_rohm tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_cd.to_sql('hi_ais_cd', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_cd)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_cd.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbd196",
   "metadata": {},
   "source": [
    "# IMPORTAR CD Detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25e4c805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CD', 'H ICH+ICHL pF', 'H ICH pF', 'H ICHL pF', 'H ICHT pF', 'X ICL+ICLT pF', 'X ICL pF', 'X ICLT pF', 'X ICLH pF', 'Y ICT+ICTH pF', 'Y ICT pF', 'Y ICTH pF', 'Y ICTL pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME ACE :\n",
    "from CD import get_df_detalles_ext_CD\n",
    "\n",
    "df_ais_cd_ext = get_df_detalles_ext_CD()\n",
    "\n",
    "# Conexión con SQLAlchemy (MUCHO más simple)\n",
    "try:\n",
    "    # Codificar contraseña por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexión\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el índice del DataFrame como columna\n",
    "    df_ais_cd_ext.to_sql('hi_ais_cd_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¡Éxito! {len(df_ais_cd_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_ais_cd_ext.columns))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b69a3a",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_fpbc1 detallado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d64871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'FPBC1', 'C1 H1 %', 'C1 H2 %', 'C1 H3 %', 'C1 H0 %']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPBC1 :\n",
    "from FPBC1 import get_df_detalles_FP_C1\n",
    "df_bus_fpbc1 = get_df_detalles_FP_C1()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpbc1.to_sql('hi_bus_fpbc1', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpbc1)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpbc1.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510cb3b",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_fpbc1 detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a648466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'FPBC1', 'C1 H1 %', 'C1 H2 %', 'C1 H3 %', 'C1 H0 %']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPBC1 EXTENDIDO :\n",
    "from FPBC1 import get_df_detalles_ext_FP_C1\n",
    "df_bus_fpbc1_ext = get_df_detalles_ext_FP_C1()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpbc1_ext.to_sql('hi_bus_fpbc1_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpbc1_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpbc1_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5449e",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_fpbc2 detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af84d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'FPBC2', 'C2 H1 %', 'C2 H2 %', 'C2 H3 %', 'C2 H0 %']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPBC2 :\n",
    "from FPBC2 import get_df_detalles_FP_C2\n",
    "df_bus_fpbc2 = get_df_detalles_FP_C2()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpbc2.to_sql('hi_bus_fpbc2', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpbc2)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpbc2.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29902f",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_fpbc2 detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dd2b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'FPBC2', 'C2 H1 %', 'C2 H2 %', 'C2 H3 %', 'C2 H0 %']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPBC2 EXTENDIDO :\n",
    "from FPBC2 import get_df_detalles_ext_FP_C2\n",
    "df_bus_fpbc2_ext = get_df_detalles_ext_FP_C2()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpbc2_ext.to_sql('hi_bus_fpbc2_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpbc2_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpbc2_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59bb5d",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cbc1 detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "883b90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CBC1', 'C1 H1 pF', 'C1 H2 pF', 'C1 H3 pF', 'C1 H0 pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME CBC1 :\n",
    "from CBC1 import get_df_detalles_C_C1\n",
    "df_bus_cbc1 = get_df_detalles_C_C1()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_cbc1.to_sql('hi_bus_cbc1', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_cbc1)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_cbc1.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ffebf",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cbc1 detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "760b7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CBC1', 'C1 H1 pF', 'C1 H2 pF', 'C1 H3 pF', 'C1 H0 pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME CBC1 EXTENDIDO :\n",
    "from CBC1 import get_df_detalles_ext_C_C1\n",
    "df_bus_cbc1_ext = get_df_detalles_ext_C_C1()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_cbc1_ext.to_sql('hi_bus_cbc1_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_cbc1_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_cbc1_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3b2cb",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cbc2 detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "129b3daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CBC2', 'C2 H1 pF', 'C2 H2 pF', 'C2 H3 pF', 'C2 NH0 pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME CBC2 :\n",
    "from CBC2 import get_df_detalles_C_C2\n",
    "df_bus_cbc2 = get_df_detalles_C_C2()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_cbc2.to_sql('hi_bus_cbc2', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_cbc2)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_cbc2.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c06ad7",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cbc2 detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93d497ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CBC2', 'C2 H1 pF', 'C2 H2 pF', 'C2 H3 pF', 'C2 NH0 pF']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME CBC2 EXTENDIDO :\n",
    "from CBC2 import get_df_detalles_ext_C_C2\n",
    "df_bus_cbc2_ext = get_df_detalles_ext_C_C2()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_cbc2_ext.to_sql('hi_bus_cbc2_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_cbc2_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_cbc2_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52f0ff",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cc extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e33ae922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CC', 'X1 mW', 'X2 mW', 'X3 mW', 'X0 mW', 'Y1 mW', 'Y2 mW', 'Y3 mW', 'Y0 mW']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPCOCA :\n",
    "from FPCOCA import get_df_detalles_CC\n",
    "df_bus_fpcoca = get_df_detalles_CC()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpcoca.to_sql('hi_bus_cc', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpcoca)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpcoca.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfafc1",
   "metadata": {},
   "source": [
    "# IMPORTAR hi_bus_cc detallado y extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10848e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'CC', 'X1 mW', 'X2 mW', 'X3 mW', 'X0 mW', 'Y1 mW', 'Y2 mW', 'Y3 mW', 'Y0 mW']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME FPCOCA EXTENDIDO :\n",
    "from FPCOCA import get_df_detalles_ext_CC\n",
    "df_bus_fpcoca_ext = get_df_detalles_ext_CC()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_bus_fpcoca_ext.to_sql('hi_bus_cc_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_bus_fpcoca_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_bus_fpcoca_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19677351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'IEX', 'R POS1 mA', 'R POS2 mA', 'R POS3 mA', 'R POS4 mA', 'R POS5 mA', 'R POS6 mA', 'R POS7 mA', 'R POS8 mA', 'R POS9 mA', 'R POS10 mA', 'R POS11 mA', 'R POS12  mA', 'R POS13 mA', 'R POS14 mA', 'R POS15 mA', 'R POS16 mA', 'R POS17 mA', 'R POS18 mA', 'R POS19 mA', 'R POS20 mA', 'R POS21 mA', 'R POS22 mA', 'R POS23 mA', 'R POS24 mA', 'R POS25 mA', 'R POS26 mA', 'R POS27 mA', 'S POS1 mA', 'S POS2 mA', 'S POS3 mA', 'S POS4 mA', 'S POS5 mA', 'S POS6 mA', 'S POS7 mA', 'S POS8 mA', 'S POS9 mA', 'S POS10 mA', 'S POS11 mA', 'S POS12  mA', 'S POS13 mA', 'S POS14 mA', 'S POS15 mA', 'S POS16 mA', 'S POS17 mA', 'S POS18 mA', 'S POS19 mA', 'S POS20 mA', 'S POS21 mA', 'S POS22 mA', 'S POS23 mA', 'S POS24 mA', 'S POS25 mA', 'S POS26 mA', 'S POS27 mA', 'T POS1 mA', 'T POS2 mA', 'T POS3 mA', 'T POS4 mA', 'T POS5 mA', 'T POS6 mA', 'T POS7 mA', 'T POS8 mA', 'T POS9 mA', 'T POS10 mA', 'T POS11 mA', 'T POS12  mA', 'T POS13 mA', 'T POS14 mA', 'T POS15 mA', 'T POS16 mA', 'T POS17 mA', 'T POS18 mA', 'T POS19 mA', 'T POS20 mA', 'T POS21 mA', 'T POS22 mA', 'T POS23 mA', 'T POS24 mA', 'T POS25 mA', 'T POS26 mA', 'T POS27 mA']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUCiex :\n",
    "from NUCiex import get_df_detalles_IEX\n",
    "df_nuc_iex = get_df_detalles_IEX()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_nuc_iex.to_sql('hi_nuc_iex', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_nuc_iex)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc_iex.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3624a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'IEX', 'R POS1 mA', 'R POS2 mA', 'R POS3 mA', 'R POS4 mA', 'R POS5 mA', 'R POS6 mA', 'R POS7 mA', 'R POS8 mA', 'R POS9 mA', 'R POS10 mA', 'R POS11 mA', 'R POS12  mA', 'R POS13 mA', 'R POS14 mA', 'R POS15 mA', 'R POS16 mA', 'R POS17 mA', 'R POS18 mA', 'R POS19 mA', 'R POS20 mA', 'R POS21 mA', 'R POS22 mA', 'R POS23 mA', 'R POS24 mA', 'R POS25 mA', 'R POS26 mA', 'R POS27 mA', 'S POS1 mA', 'S POS2 mA', 'S POS3 mA', 'S POS4 mA', 'S POS5 mA', 'S POS6 mA', 'S POS7 mA', 'S POS8 mA', 'S POS9 mA', 'S POS10 mA', 'S POS11 mA', 'S POS12  mA', 'S POS13 mA', 'S POS14 mA', 'S POS15 mA', 'S POS16 mA', 'S POS17 mA', 'S POS18 mA', 'S POS19 mA', 'S POS20 mA', 'S POS21 mA', 'S POS22 mA', 'S POS23 mA', 'S POS24 mA', 'S POS25 mA', 'S POS26 mA', 'S POS27 mA', 'T POS1 mA', 'T POS2 mA', 'T POS3 mA', 'T POS4 mA', 'T POS5 mA', 'T POS6 mA', 'T POS7 mA', 'T POS8 mA', 'T POS9 mA', 'T POS10 mA', 'T POS11 mA', 'T POS12  mA', 'T POS13 mA', 'T POS14 mA', 'T POS15 mA', 'T POS16 mA', 'T POS17 mA', 'T POS18 mA', 'T POS19 mA', 'T POS20 mA', 'T POS21 mA', 'T POS22 mA', 'T POS23 mA', 'T POS24 mA', 'T POS25 mA', 'T POS26 mA', 'T POS27 mA']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUCiex EXTENDIDO :\n",
    "from NUCiex import get_df_detalles_ext_IEX\n",
    "df_nuc_iex_ext = get_df_detalles_ext_IEX()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_nuc_iex_ext.to_sql('hi_nuc_iex_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_nuc_iex_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc_iex_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89d42159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 16 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RNUC', 'Valor (MΩ)']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUCrnuc :\n",
    "from NUCrnuc import get_df_detalles_RNUC\n",
    "df_nuc_rnuc = get_df_detalles_RNUC()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_nuc_rnuc.to_sql('hi_nuc_rnuc', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_nuc_rnuc)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc_rnuc.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8878c105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Éxito! 15728 registros importados automáticamente\n",
      "La tabla se creó con las columnas: ['SERIE', 'FECHA', 'RNUC', 'Valor (MΩ)']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME NUCrnuc EXTENDIDO :\n",
    "from NUCrnuc import get_df_detalles_ext_RNUC\n",
    "df_nuc_rnuc_ext = get_df_detalles_ext_RNUC()\n",
    "try:\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "    df_nuc_rnuc_ext.to_sql('hi_nuc_rnuc_extendido', engine, if_exists='replace', index=False, schema='raw_v2',dtype={'FECHA': Date()})\n",
    "    print(f\"¡Éxito! {len(df_nuc_rnuc_ext)} registros importados automáticamente\")\n",
    "    print(\"La tabla se creó con las columnas:\", list(df_nuc_rnuc_ext.columns))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
