{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcda64d",
   "metadata": {},
   "source": [
    "### Importaci√≥n de librer√≠as y direcci√≥n del path donde est√°n los scripts de .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c867c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "from sqlalchemy.types import Date\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\roquispec\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\")\n",
    "sys.path.append(r\"C:\\Users\\RONALD Q\\OneDrive - LUZ DEL SUR S.A.A\\Documentos\\Estudios de Ingreso\\ProyectoRyD_V2\\ScriptsPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026901f2",
   "metadata": {},
   "source": [
    "### Carga autom√°tica de tabla de √≠ndice generla(HI) a PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca8643",
   "metadata": {},
   "source": [
    "Los carga autom√°ticamente en el esquema 'processed_v2' con nombre de tabla 'hi_general'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd75987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DGA] columnas: ['SERIE', 'FECHA', 'DGA']\n",
      "[ACE] columnas: ['SERIE', 'FECHA', 'ACE']\n",
      "[ARR] columnas: ['SERIE', 'FECHA', 'ARR']\n",
      "[AIS] columnas: ['SERIE', 'FECHA', 'AIS']\n",
      "[NUC] columnas: ['SERIE', 'FECHA', 'NUC']\n",
      "[OLTC] columnas: ['SERIE', 'FECHA', 'OLTC']\n",
      "[BUS] columnas: ['SERIE', 'FECHA', 'BUS']\n",
      "SERIE            object\n",
      "FECHA    datetime64[ns]\n",
      "HI              float64\n",
      "DGA             float64\n",
      "ACE             float64\n",
      "ARR             float64\n",
      "AIS             float64\n",
      "NUC             float64\n",
      "OLTC            float64\n",
      "BUS             float64\n",
      "dtype: object\n",
      "       SERIE      FECHA        HI   DGA       ACE  ARR       AIS  NUC  OLTC  \\\n",
      "0  230531-01 2025-10-22  3.910714  2.80  4.000000  5.0  4.142857  3.5   3.5   \n",
      "1   146660T3 2025-10-22  3.862347  2.65  4.090909  5.0  3.857143  3.5   5.0   \n",
      "2     338118 2025-10-22  3.584578  2.35  2.363636  5.0  4.142857  3.5   2.5   \n",
      "\n",
      "        BUS  \n",
      "0  5.000000  \n",
      "1  4.529412  \n",
      "2  5.000000  \n",
      "¬°√âxito! 15792 registros importados autom√°ticamente\n",
      "La tabla se cre√≥ con las columnas: ['SERIE', 'FECHA', 'HI', 'DGA', 'ACE', 'ARR', 'AIS', 'NUC', 'OLTC', 'BUS']\n"
     ]
    }
   ],
   "source": [
    "# IMPORTAR DATAFRAME HI:\n",
    "from main import obtener_HI\n",
    "\n",
    "df_HI = obtener_HI()\n",
    "df_HI = df_HI.sort_values(by=[\"FECHA\", \"HI\"], ascending=[False, False]).reset_index(drop=True)\n",
    "print(df_HI.dtypes)\n",
    "print(df_HI.head(3))\n",
    "\n",
    "# Conexi√≥n con SQLAlchemy (MUCHO m√°s simple)\n",
    "try:\n",
    "    # Codificar contrase√±a por si tiene caracteres especiales\n",
    "    password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "    \n",
    "    # Crear engine de conexi√≥n\n",
    "    engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "        \n",
    "    # Importar directamente el DataFrame a PostgreSQL\n",
    "    # if_exists='replace' = borra la tabla si existe y crea una nueva\n",
    "    # index=False = no incluir el √≠ndice del DataFrame como columna\n",
    "    df_HI.to_sql('hi_general', engine, if_exists='replace', index=False, schema='processed_v2',dtype={'FECHA': Date()})\n",
    "    \n",
    "    print(f\"¬°√âxito! {len(df_HI)} registros importados autom√°ticamente\")\n",
    "    print(\"La tabla se cre√≥ con las columnas:\", list(df_HI.columns))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e7586",
   "metadata": {},
   "source": [
    "### Carga autom√°tica de tablas de sub√≠ndices (DGA,ACE,AIS,ARR,NUC,OLTC,BUS) a PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886751c",
   "metadata": {},
   "source": [
    "Los carga autom√°ticamente en el esquema 'raw_v2' con nombres de tabla 'hi_<sub√≠ndice>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb90b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib\n",
    "# from sqlalchemy import create_engine, Date\n",
    "\n",
    "# # ================================\n",
    "# # 1Ô∏è‚É£ Definir m√≥dulos y funciones\n",
    "# # ================================\n",
    "# from DGA import get_df_detalles_DGA\n",
    "# from ACE import get_df_detalles_ACE\n",
    "# from ARR import get_df_detalles_ARR\n",
    "# from AIS import get_df_detalles_AIS\n",
    "# from NUC import get_df_detalles_NUC\n",
    "# from OLTC import get_df_detalles_OLTC\n",
    "# from BUS import get_df_detalles_BUS\n",
    "\n",
    "# # Diccionario con las funciones de obtenci√≥n de DataFrames\n",
    "# data_sources = {\n",
    "#     \"dga\": get_df_detalles_DGA,\n",
    "#     \"ace\": get_df_detalles_ACE,\n",
    "#     \"arr\": get_df_detalles_ARR,\n",
    "#     \"ais\": get_df_detalles_AIS,\n",
    "#     \"nuc\": get_df_detalles_NUC,\n",
    "#     \"oltc\": get_df_detalles_OLTC,\n",
    "#     \"bus\": get_df_detalles_BUS,\n",
    "# }\n",
    "\n",
    "# # ================================\n",
    "# # 2Ô∏è‚É£ Conexi√≥n SQL\n",
    "# # ================================\n",
    "# password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "# engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "\n",
    "# # ================================\n",
    "# # 3Ô∏è‚É£ Cargar todos los DataFrames\n",
    "# # ================================\n",
    "# for name, func in data_sources.items():\n",
    "#     try:\n",
    "#         df = func()\n",
    "#         df.to_sql(\n",
    "#             f\"hi_{name}\", \n",
    "#             engine, \n",
    "#             if_exists='replace', \n",
    "#             index=False, \n",
    "#             schema='raw_v2',\n",
    "#             dtype={'FECHA': Date()}\n",
    "#         )\n",
    "#         print(f\"‚úÖ ¬°√âxito! {len(df)} registros importados a hi_{name}\")\n",
    "#         print(\"   Columnas:\", list(df.columns))\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error al importar {name.upper()}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d54387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No hay registros nuevos para hi_dga\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_ace\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_arr\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_ais\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_nuc\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_oltc\n",
      "‚ÑπÔ∏è No hay registros nuevos para hi_bus\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from sqlalchemy import create_engine, Date, text\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Definir m√≥dulos y funciones\n",
    "# ================================\n",
    "from DGA import get_df_detalles_DGA\n",
    "from ACE import get_df_detalles_ACE\n",
    "from ARR import get_df_detalles_ARR\n",
    "from AIS import get_df_detalles_AIS\n",
    "from NUC import get_df_detalles_NUC\n",
    "from OLTC import get_df_detalles_OLTC\n",
    "from BUS import get_df_detalles_BUS\n",
    "\n",
    "# Diccionario con las funciones de obtenci√≥n de DataFrames\n",
    "data_sources = {\n",
    "    \"dga\": get_df_detalles_DGA,\n",
    "    \"ace\": get_df_detalles_ACE,\n",
    "    \"arr\": get_df_detalles_ARR,\n",
    "    \"ais\": get_df_detalles_AIS,\n",
    "    \"nuc\": get_df_detalles_NUC,\n",
    "    \"oltc\": get_df_detalles_OLTC,\n",
    "    \"bus\": get_df_detalles_BUS,\n",
    "}\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Conexi√≥n SQL\n",
    "# ================================\n",
    "password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Cargar solo datos nuevos por SERIE\n",
    "# ================================\n",
    "for name, func in data_sources.items():\n",
    "    try:\n",
    "        df = func()\n",
    "        table_name = f\"hi_{name}\"\n",
    "        schema_name = \"raw_v2\"\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            # Verificar si la tabla existe\n",
    "            check_table = conn.execute(text(f\"\"\"\n",
    "                SELECT EXISTS (\n",
    "                    SELECT FROM information_schema.tables \n",
    "                    WHERE table_schema = :schema AND table_name = :table\n",
    "                )\n",
    "            \"\"\"), {\"schema\": schema_name, \"table\": table_name}).scalar()\n",
    "\n",
    "            if check_table:\n",
    "                # Obtener todas las series existentes\n",
    "                series_existentes = conn.execute(text(f\"\"\"\n",
    "                    SELECT DISTINCT \"SERIE\" FROM {schema_name}.{table_name}\n",
    "                \"\"\"))\n",
    "                series_existentes = [row[0] for row in series_existentes]\n",
    "\n",
    "                nuevos_registros = pd.DataFrame()\n",
    "\n",
    "                for serie in df[\"SERIE\"].unique():\n",
    "                    df_serie = df[df[\"SERIE\"] == serie]\n",
    "                    if serie in series_existentes:\n",
    "                        max_fecha = conn.execute(text(f\"\"\"\n",
    "                            SELECT MAX(\"FECHA\") FROM {schema_name}.{table_name} WHERE \"SERIE\" = :serie\n",
    "                        \"\"\"), {\"serie\": serie}).scalar()\n",
    "                        df_serie = df_serie[df_serie[\"FECHA\"] > pd.to_datetime(max_fecha)]\n",
    "                    nuevos_registros = pd.concat([nuevos_registros, df_serie], ignore_index=True)\n",
    "\n",
    "                if not nuevos_registros.empty:\n",
    "                    nuevos_registros.to_sql(\n",
    "                        table_name,\n",
    "                        engine,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        schema=schema_name,\n",
    "                        dtype={'FECHA': Date()}\n",
    "                    )\n",
    "                    print(f\"‚úÖ {len(nuevos_registros)} registros nuevos a√±adidos a {table_name}\")\n",
    "                else:\n",
    "                    print(f\"‚ÑπÔ∏è No hay registros nuevos para {table_name}\")\n",
    "            else:\n",
    "                # Si la tabla no existe, crearla con todos los datos\n",
    "                df.to_sql(\n",
    "                    table_name,\n",
    "                    engine,\n",
    "                    if_exists='replace',\n",
    "                    index=False,\n",
    "                    schema=schema_name,\n",
    "                    dtype={'FECHA': Date()}\n",
    "                )\n",
    "                print(f\"üÜï Tabla {table_name} creada con {len(df)} registros\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al importar {name.upper()}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b037848",
   "metadata": {},
   "source": [
    "### Carga autom√°tica de tablas de sub√≠ndices (DGA,ACE,AIS,ARR,NUC,OLTC,BUS) extendidos a PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9e53b",
   "metadata": {},
   "source": [
    "Los carga autom√°ticamente en el esquema 'raw_v2' con nombres de tabla 'hi_<sub√≠ndice>_extendido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b87ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_dga_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'DGA', 'H2', 'CH4', 'C2H2', 'C2H4', 'C2H6', 'CO', 'CO2', 'O2']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_ace_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'ACE', 'FP25', 'FP100', 'HU', 'AC', 'TIF', 'CO', 'RD', 'IO']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_arr_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'ARR', 'ROHM', 'RTRA', 'RDIS']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_ais_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'AIS', 'FPDEVANADO', 'CD', 'FUR']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_nuc_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'NUC', 'RNUC', 'IEX']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_oltc_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'OLTC', 'RD', 'H20']\n",
      "‚úÖ ¬°√âxito! 15792 registros importados a hi_bus_extendido\n",
      "   Columnas: ['SERIE', 'FECHA', 'BUS', 'FPBC1', 'FPBC2', 'CBC1', 'CBC2', 'CC']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from sqlalchemy import create_engine, Date\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Importar funciones extendidas\n",
    "# ================================\n",
    "from DGA import get_df_detalles_ext_DGA\n",
    "from ACE import get_df_detalles_ext_ACE\n",
    "from ARR import get_df_detalles_ext_ARR\n",
    "from AIS import get_df_detalles_ext_AIS\n",
    "from NUC import get_df_detalles_ext_NUC\n",
    "from OLTC import get_df_detalles_ext_OLTC\n",
    "from BUS import get_df_detalles_ext_BUS\n",
    "\n",
    "# Diccionario con las funciones extendidas\n",
    "data_sources_ext = {\n",
    "    \"dga_extendido\": get_df_detalles_ext_DGA,\n",
    "    \"ace_extendido\": get_df_detalles_ext_ACE,\n",
    "    \"arr_extendido\": get_df_detalles_ext_ARR,\n",
    "    \"ais_extendido\": get_df_detalles_ext_AIS,\n",
    "    \"nuc_extendido\": get_df_detalles_ext_NUC,\n",
    "    \"oltc_extendido\": get_df_detalles_ext_OLTC,\n",
    "    \"bus_extendido\": get_df_detalles_ext_BUS,\n",
    "}\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Conexi√≥n SQL\n",
    "# ================================\n",
    "password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Cargar todos los DataFrames extendidos\n",
    "# ================================\n",
    "for name, func in data_sources_ext.items():\n",
    "    try:\n",
    "        df = func()\n",
    "        df.to_sql(\n",
    "            f\"hi_{name}\",\n",
    "            engine,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            schema='raw_v2',\n",
    "            dtype={'FECHA': Date()}\n",
    "        )\n",
    "        print(f\"‚úÖ ¬°√âxito! {len(df)} registros importados a hi_{name}\")\n",
    "        print(\"   Columnas:\", list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al importar {name.upper()}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "412134a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib\n",
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, Date, text\n",
    "# from datetime import datetime\n",
    "\n",
    "# # ================================\n",
    "# # 1Ô∏è‚É£ Importar funciones extendidas\n",
    "# # ================================\n",
    "# from DGA import get_df_detalles_ext_DGA\n",
    "# from ACE import get_df_detalles_ext_ACE\n",
    "# from ARR import get_df_detalles_ext_ARR\n",
    "# from AIS import get_df_detalles_ext_AIS\n",
    "# from NUC import get_df_detalles_ext_NUC\n",
    "# from OLTC import get_df_detalles_ext_OLTC\n",
    "# from BUS import get_df_detalles_ext_BUS\n",
    "\n",
    "# # Diccionario con las funciones extendidas\n",
    "# data_sources_ext = {\n",
    "#     \"dga_extendido\": get_df_detalles_ext_DGA,\n",
    "#     \"ace_extendido\": get_df_detalles_ext_ACE,\n",
    "#     \"arr_extendido\": get_df_detalles_ext_ARR,\n",
    "#     \"ais_extendido\": get_df_detalles_ext_AIS,\n",
    "#     \"nuc_extendido\": get_df_detalles_ext_NUC,\n",
    "#     \"oltc_extendido\": get_df_detalles_ext_OLTC,\n",
    "#     \"bus_extendido\": get_df_detalles_ext_BUS,\n",
    "# }\n",
    "\n",
    "# # ================================\n",
    "# # 2Ô∏è‚É£ Conexi√≥n SQL\n",
    "# # ================================\n",
    "# password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "# engine = create_engine(f'postgresql+psycopg2://postgres:{password}@localhost:5432/postgres')\n",
    "\n",
    "# # ================================\n",
    "# # 3Ô∏è‚É£ Funci√≥n para carga incremental\n",
    "# # ================================\n",
    "# def carga_incremental(tabla_nombre, funcion_fuente, engine):\n",
    "#     \"\"\"\n",
    "#     Realiza carga incremental comparando datos existentes con nuevos\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Obtener nuevos datos\n",
    "#         nuevos_datos = funcion_fuente()\n",
    "        \n",
    "#         if nuevos_datos.empty:\n",
    "#             print(f\"‚ö†Ô∏è No hay nuevos datos para {tabla_nombre}\")\n",
    "#             return\n",
    "        \n",
    "#         # Verificar si la tabla existe en la base de datos\n",
    "#         with engine.connect() as conn:\n",
    "#             tabla_existe = conn.execute(text(f\"\"\"\n",
    "#                 SELECT EXISTS (\n",
    "#                     SELECT FROM information_schema.tables \n",
    "#                     WHERE table_schema = 'raw_v2' \n",
    "#                     AND table_name = 'hi_{tabla_nombre}'\n",
    "#                 );\n",
    "#             \"\"\")).scalar()\n",
    "        \n",
    "#         if not tabla_existe:\n",
    "#             # Si la tabla no existe, crear con todos los datos\n",
    "#             nuevos_datos.to_sql(\n",
    "#                 f\"hi_{tabla_nombre}\",\n",
    "#                 engine,\n",
    "#                 if_exists='fail',\n",
    "#                 index=False,\n",
    "#                 schema='raw_v2',\n",
    "#                 dtype={'FECHA': Date()}\n",
    "#             )\n",
    "#             print(f\"‚úÖ ¬°Tabla creada! {len(nuevos_datos)} registros importados a hi_{tabla_nombre}\")\n",
    "#             return\n",
    "        \n",
    "#         # Si la tabla existe, obtener datos existentes para comparar\n",
    "#         with engine.connect() as conn:\n",
    "#             datos_existentes = pd.read_sql_table(\n",
    "#                 f\"hi_{tabla_nombre}\", \n",
    "#                 conn, \n",
    "#                 schema='raw_v2'\n",
    "#             )\n",
    "        \n",
    "#         if datos_existentes.empty:\n",
    "#             # Si la tabla existe pero est√° vac√≠a, insertar todos los datos\n",
    "#             nuevos_datos.to_sql(\n",
    "#                 f\"hi_{tabla_nombre}\",\n",
    "#                 engine,\n",
    "#                 if_exists='append',\n",
    "#                 index=False,\n",
    "#                 schema='raw_v2'\n",
    "#             )\n",
    "#             print(f\"‚úÖ ¬°Datos cargados en tabla vac√≠a! {len(nuevos_datos)} registros importados a hi_{tabla_nombre}\")\n",
    "#             return\n",
    "        \n",
    "#         # Identificar duplicados basado en todas las columnas\n",
    "#         # Combinar datos existentes y nuevos\n",
    "#         todos_datos = pd.concat([datos_existentes, nuevos_datos], ignore_index=True)\n",
    "        \n",
    "#         # Eliminar duplicados manteniendo los primeros (los existentes)\n",
    "#         datos_sin_duplicados = todos_datos.drop_duplicates(keep='first')\n",
    "        \n",
    "#         # Filtrar solo los registros nuevos (los que no estaban en existentes)\n",
    "#         datos_nuevos = datos_sin_duplicados.iloc[len(datos_existentes):]\n",
    "        \n",
    "#         if datos_nuevos.empty:\n",
    "#             print(f\"‚ÑπÔ∏è No hay datos nuevos para {tabla_nombre}. Tabla actualizada.\")\n",
    "#             return\n",
    "        \n",
    "#         # Insertar solo los datos nuevos\n",
    "#         datos_nuevos.to_sql(\n",
    "#             f\"hi_{tabla_nombre}\",\n",
    "#             engine,\n",
    "#             if_exists='append',\n",
    "#             index=False,\n",
    "#             schema='raw_v2'\n",
    "#         )\n",
    "        \n",
    "#         print(f\"‚úÖ ¬°Carga incremental exitosa! {len(datos_nuevos)} nuevos registros a√±adidos a hi_{tabla_nombre}\")\n",
    "#         print(f\"   Total en tabla: {len(datos_sin_duplicados)} registros\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error en carga incremental para {tabla_nombre}: {e}\")\n",
    "\n",
    "# # ================================\n",
    "# # 4Ô∏è‚É£ Ejecutar carga incremental para todas las tablas\n",
    "# # ================================\n",
    "# for name, func in data_sources_ext.items():\n",
    "#     print(f\"\\nüîÑ Procesando: {name}\")\n",
    "#     carga_incremental(name, func, engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc32238",
   "metadata": {},
   "source": [
    "### Carga autom√°tica de tabla de Sub-sub√≠ndices detallados y extendidos a PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73201da0",
   "metadata": {},
   "source": [
    "Los carga autom√°ticamente en el esquema 'raw_v2' con nombre de tabla 'hi_<sub√≠ndice>_ <subsub√≠ndice>' y 'hi_<sub√≠ndice>_<subsub√≠ndice>_extendido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ hi_arr_rohm: 16 registros importados correctamente.\n",
      "   Columnas: ['SERIE', 'FECHA', 'ROHM', 'H POS1 R 75¬∞C [mŒ©]', 'H POS2 R 75¬∞C [mŒ©]', 'H POS3 R 75¬∞C [mŒ©]', 'H POS4 R 75¬∞C [mŒ©]', 'H POS5 R 75¬∞C [mŒ©]', 'H POS6 R 75¬∞C [mŒ©]', 'H POS7 R 75¬∞C [mŒ©]', 'H POS8 R 75¬∞C [mŒ©]', 'H POS9 R 75¬∞C [mŒ©]', 'H POS10 R 75¬∞C [mŒ©]', 'H POS11 R 75¬∞C [mŒ©]', 'H POS12 R 75¬∞C [mŒ©]', 'H POS13 R 75¬∞C [mŒ©]', 'H POS14 R 75¬∞C [mŒ©]', 'H POS15 R 75¬∞C [mŒ©]', 'H POS16 R 75¬∞C [mŒ©]', 'H POS17 R 75¬∞C [mŒ©]', 'H POS18 R 75¬∞C [mŒ©]', 'H POS19 R 75¬∞C [mŒ©]', 'H POS20 R 75¬∞C [mŒ©]', 'H POS21 R 75¬∞C [mŒ©]', 'H POS22 R 75¬∞C [mŒ©]', 'H POS23 R 75¬∞C [mŒ©]', 'H POS24 R 75¬∞C [mŒ©]', 'H POS25 R 75¬∞C [mŒ©]', 'H POS26 R 75¬∞C [mŒ©]', 'H POS27 R 75¬∞C [mŒ©]', 'H POS1 S 75¬∞C [mŒ©]', 'H POS2 S 75¬∞C [mŒ©]', 'H POS3 S 75¬∞C [mŒ©]', 'H POS4 S 75¬∞C [mŒ©]', 'H POS5 S 75¬∞C [mŒ©]', 'H POS6 S 75¬∞C [mŒ©]', 'H POS7 S 75¬∞C [mŒ©]', 'H POS8 S 75¬∞C [mŒ©]', 'H POS9 S 75¬∞C [mŒ©]', 'H POS10 S 75¬∞C [mŒ©]', 'H POS11 S 75¬∞C [mŒ©]', 'H POS12 S 75¬∞C [mŒ©]', 'H POS13 S 75¬∞C [mŒ©]', 'H POS14 S 75¬∞C [mŒ©]', 'H POS15 S 75¬∞C [mŒ©]', 'H POS16 S 75¬∞C [mŒ©]', 'H POS17 S 75¬∞C [mŒ©]', 'H POS18 S 75¬∞C [mŒ©]', 'H POS19 S 75¬∞C [mŒ©]', 'H POS20 S 75¬∞C [mŒ©]', 'H POS21 S 75¬∞C [mŒ©]', 'H POS22 S 75¬∞C [mŒ©]', 'H POS23 S 75¬∞C [mŒ©]', 'H POS24 S 75¬∞C [mŒ©]', 'H POS25 S 75¬∞C [mŒ©]', 'H POS26 S 75¬∞C [mŒ©]', 'H POS27 S 75¬∞C [mŒ©]', 'H POS1 T 75¬∞C [mŒ©]', 'H POS2 T 75¬∞C [mŒ©]', 'H POS3 T 75¬∞C [mŒ©]', 'H POS4 T 75¬∞C [mŒ©]', 'H POS5 T 75¬∞C [mŒ©]', 'H POS6 T 75¬∞C [mŒ©]', 'H POS7 T 75¬∞C [mŒ©]', 'H POS8 T 75¬∞C [mŒ©]', 'H POS9 T 75¬∞C [mŒ©]', 'H POS10 T 75¬∞C [mŒ©]', 'H POS11 T 75¬∞C [mŒ©]', 'H POS12 T 75¬∞C [mŒ©]', 'H POS13 T 75¬∞C [mŒ©]', 'H POS14 T 75¬∞C [mŒ©]', 'H POS15 T 75¬∞C [mŒ©]', 'H POS16 T 75¬∞C [mŒ©]', 'H POS17 T 75¬∞C [mŒ©]', 'H POS18 T 75¬∞C [mŒ©]', 'H POS19 T 75¬∞C [mŒ©]', 'H POS20 T 75¬∞C [mŒ©]', 'H POS21 T 75¬∞C [mŒ©]', 'H POS22 T 75¬∞C [mŒ©]', 'H POS23 T 75¬∞C [mŒ©]', 'H POS24 T 75¬∞C [mŒ©]', 'H POS25 T 75¬∞C [mŒ©]', 'H POS26 T 75¬∞C [mŒ©]', 'H POS27 T 75¬∞C [mŒ©]', 'X POS1 R 75¬∞C[mŒ©]', 'X POS1 S 75¬∞C[mŒ©]', 'X POS1 T 75¬∞C[mŒ©]', 'Y POS1 R 75¬∞C[mŒ©]', 'Y POS1 S 75¬∞C[mŒ©]', 'Y POS1 T 75¬∞C[mŒ©]']\n",
      "‚úÖ hi_arr_rtra: 16 registros importados correctamente.\n",
      "   Columnas: ['SERIE', 'FECHA', 'RTRA', 'H-X POS1 R', 'H-X POS2 R', 'H-X POS3 R', 'H-X POS4 R', 'H-X POS5 R', 'H-X POS6 R', 'H-X POS7 R', 'H-X POS8 R', 'H-X POS9 R', 'H-X POS10 R', 'H-X POS11 R', 'H-X POS12 R', 'H-X POS13 R', 'H-X POS14 R', 'H-X POS15 R', 'H-X POS16 R', 'H-X POS17 R', 'H-X POS18 R', 'H-X POS19 R', 'H-X POS20 R', 'H-X POS21 R', 'H-X POS22 R', 'H-X POS23 R', 'H-X POS24 R', 'H-X POS25 R', 'H-X POS26 R', 'H-X POS27 R', 'H-X POS1 S', 'H-X POS2 S', 'H-X POS3 S', 'H-X POS4 S', 'H-X POS5 S', 'H-X POS6 S', 'H-X POS7 S', 'H-X POS8 S', 'H-X POS9 S', 'H-X POS10 S', 'H-X POS11 S', 'H-X POS12 S', 'H-X POS13 S', 'H-X POS14 S', 'H-X POS15 S', 'H-X POS16 S', 'H-X POS17 S', 'H-X POS18 S', 'H-X POS19 S', 'H-X POS20 S', 'H-X POS21 S', 'H-X POS22 S', 'H-X POS23 S', 'H-X POS24 S', 'H-X POS25 S', 'H-X POS26 S', 'H-X POS27 S', 'H-X POS1 T', 'H-X POS2 T', 'H-X POS3 T', 'H-X POS4 T', 'H-X POS5 T', 'H-X POS6 T', 'H-X POS7 T', 'H-X POS8 T', 'H-X POS9 T', 'H-X POS10 T', 'H-X POS11 T', 'H-X POS12 T', 'H-X POS13 T', 'H-X POS14 T', 'H-X POS15 T', 'H-X POS16 T', 'H-X POS17 T', 'H-X POS18 T', 'H-X POS19 T', 'H-X POS20 T', 'H-X POS21 T', 'H-X POS22 T', 'H-X POS23 T', 'H-X POS24 T', 'H-X POS25 T', 'H-X POS26 T', 'H-X POS27 T', 'H-Y POS1 R', 'H-Y POS2 R', 'H-Y POS3 R', 'H-Y POS4 R', 'H-Y POS5 R', 'H-Y POS6 R', 'H-Y POS7 R', 'H-Y POS8 R', 'H-Y POS9 R', 'H-Y POS10 R', 'H-Y POS11 R', 'H-Y POS12 R', 'H-Y POS13 R', 'H-Y POS14 R', 'H-Y POS15 R', 'H-Y POS16 R', 'H-Y POS17 R', 'H-Y POS18 R', 'H-Y POS19 R', 'H-Y POS20 R', 'H-Y POS21 R', 'H-Y POS22 R', 'H-Y POS23 R', 'H-Y POS24 R', 'H-Y POS25 R', 'H-Y POS26 R', 'H-Y POS27 R', 'H-Y POS1 S', 'H-Y POS2 S', 'H-Y POS3 S', 'H-Y POS4 S', 'H-Y POS5 S', 'H-Y POS6 S', 'H-Y POS7 S', 'H-Y POS8 S', 'H-Y POS9 S', 'H-Y POS10 S', 'H-Y POS11 S', 'H-Y POS12 S', 'H-Y POS13 S', 'H-Y POS14 S', 'H-Y POS15 S', 'H-Y POS16 S', 'H-Y POS17 S', 'H-Y POS18 S', 'H-Y POS19 S', 'H-Y POS20 S', 'H-Y POS21 S', 'H-Y POS22 S', 'H-Y POS23 S', 'H-Y POS24 S', 'H-Y POS25 S', 'H-Y POS26 S', 'H-Y POS27 S', 'H-Y POS1 T', 'H-Y POS2 T', 'H-Y POS3 T', 'H-Y POS4 T', 'H-Y POS5 T', 'H-Y POS6 T', 'H-Y POS7 T', 'H-Y POS8 T', 'H-Y POS9 T', 'H-Y POS10 T', 'H-Y POS11 T', 'H-Y POS12 T', 'H-Y POS13 T', 'H-Y POS14 T', 'H-Y POS15 T', 'H-Y POS16 T', 'H-Y POS17 T', 'H-Y POS18 T', 'H-Y POS19 T', 'H-Y POS20 T', 'H-Y POS21 T', 'H-Y POS22 T', 'H-Y POS23 T', 'H-Y POS24 T', 'H-Y POS25 T', 'H-Y POS26 T', 'H-Y POS27 T', 'X-Y POS1 R', 'X-Y POS1 S', 'X-Y POS1 T']\n",
      "‚úÖ hi_arr_rdis: 16 registros importados correctamente.\n",
      "   Columnas: ['SERIE', 'FECHA', 'RDIS', 'H-X 1 [%] ONAF 2', 'H-X 14 [%] ONAF 2', 'H-X 27 [%] ONAF 2', 'H-Y 1 [%]', 'H-Y 14 [%]', 'H-Y 27 [%]', 'X-Y 1 [%]', 'X-Y 2 [%]', 'X-Y 3 [%]']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from sqlalchemy import create_engine, Date\n",
    "\n",
    "# =====================================================\n",
    "# üîπ 1Ô∏è‚É£ Importar todos los m√≥dulos normales y extendidos\n",
    "# =====================================================\n",
    "# ARR\n",
    "from ARRrohm import get_df_detalles_ROHM, get_df_detalles_ext_ROHM\n",
    "from ARRrtra import get_df_detalles_RTRA, get_df_detalles_ext_RTRA\n",
    "from ARRdis import get_df_detalles_DIS, get_df_detalles_ext_DIS\n",
    "\n",
    "# AIS\n",
    "from FURANOS import get_df_detalles_FUR, get_df_detalles_ext_FUR\n",
    "from FP import get_df_detalles_FP, get_df_detalles_ext_FP\n",
    "from CD import get_df_detalles_CD, get_df_detalles_ext_CD\n",
    "\n",
    "# BUS\n",
    "from FPBC1 import get_df_detalles_FP_C1, get_df_detalles_ext_FP_C1\n",
    "from FPBC2 import get_df_detalles_FP_C2, get_df_detalles_ext_FP_C2\n",
    "from CBC1 import get_df_detalles_C_C1, get_df_detalles_ext_C_C1\n",
    "from CBC2 import get_df_detalles_C_C2, get_df_detalles_ext_C_C2\n",
    "from FPCOCA import get_df_detalles_CC, get_df_detalles_ext_CC\n",
    "\n",
    "# NUC\n",
    "from NUCiex import get_df_detalles_IEX, get_df_detalles_ext_IEX\n",
    "from NUCrnuc import get_df_detalles_RNUC, get_df_detalles_ext_RNUC\n",
    "\n",
    "# =====================================================\n",
    "# üîπ 2Ô∏è‚É£ Diccionario maestro de fuentes\n",
    "# =====================================================\n",
    "sources = {\n",
    "    # ARR normales\n",
    "    \"hi_arr_rohm\": get_df_detalles_ROHM,\n",
    "    \"hi_arr_rtra\": get_df_detalles_RTRA,\n",
    "    \"hi_arr_rdis\": get_df_detalles_DIS,\n",
    "\n",
    "    # ARR extendidos\n",
    "    \"hi_arr_rohm_extendido\": get_df_detalles_ext_ROHM,\n",
    "    \"hi_arr_rtra_extendido\": get_df_detalles_ext_RTRA,\n",
    "    \"hi_arr_rdis_extendido\": get_df_detalles_ext_DIS,\n",
    "\n",
    "    # AIS normales\n",
    "    \"hi_ais_furanos\": get_df_detalles_FUR,\n",
    "    \"hi_ais_fp\": get_df_detalles_FP,\n",
    "    \"hi_ais_cd\": get_df_detalles_CD,\n",
    "\n",
    "    # AIS extendidos\n",
    "    \"hi_ais_furanos_extendido\": get_df_detalles_ext_FUR,\n",
    "    \"hi_ais_fp_extendido\": get_df_detalles_ext_FP,\n",
    "    \"hi_ais_cd_extendido\": get_df_detalles_ext_CD,\n",
    "\n",
    "    # BUS normales\n",
    "    \"hi_bus_fpbc1\": get_df_detalles_FP_C1,\n",
    "    \"hi_bus_fpbc2\": get_df_detalles_FP_C2,\n",
    "    \"hi_bus_cbc1\": get_df_detalles_C_C1,\n",
    "    \"hi_bus_cbc2\": get_df_detalles_C_C2,\n",
    "    \"hi_bus_cc\": get_df_detalles_CC,\n",
    "\n",
    "    # BUS extendidos\n",
    "    \"hi_bus_fpbc1_extendido\": get_df_detalles_ext_FP_C1,\n",
    "    \"hi_bus_fpbc2_extendido\": get_df_detalles_ext_FP_C2,\n",
    "    \"hi_bus_cbc1_extendido\": get_df_detalles_ext_C_C1,\n",
    "    \"hi_bus_cbc2_extendido\": get_df_detalles_ext_C_C2,\n",
    "    \"hi_bus_cc_extendido\": get_df_detalles_ext_CC,\n",
    "\n",
    "    # NUC normales\n",
    "    \"hi_nuc_iex\": get_df_detalles_IEX,\n",
    "    \"hi_nuc_rnuc\": get_df_detalles_RNUC,\n",
    "\n",
    "    # NUC extendidos\n",
    "    \"hi_nuc_iex_extendido\": get_df_detalles_ext_IEX,\n",
    "    \"hi_nuc_rnuc_extendido\": get_df_detalles_ext_RNUC,\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# üîπ 3Ô∏è‚É£ Conexi√≥n √∫nica a PostgreSQL\n",
    "# =====================================================\n",
    "password = urllib.parse.quote_plus(\"delangeluz\")\n",
    "engine = create_engine(f\"postgresql+psycopg2://postgres:{password}@localhost:5432/postgres\")\n",
    "\n",
    "# =====================================================\n",
    "# üîπ 4Ô∏è‚É£ Cargar todos los DataFrames autom√°ticamente\n",
    "# =====================================================\n",
    "for table_name, func in sources.items():\n",
    "    try:\n",
    "        df = func()\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            engine,\n",
    "            if_exists=\"replace\",\n",
    "            index=False,\n",
    "            schema=\"raw_v2\",\n",
    "            dtype={\"FECHA\": Date()},\n",
    "        )\n",
    "        print(f\"‚úÖ {table_name}: {len(df)} registros importados correctamente.\")\n",
    "        print(\"   Columnas:\", list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al importar {table_name}: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Proceso finalizado para todos los sub√≠ndices y extendidos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
